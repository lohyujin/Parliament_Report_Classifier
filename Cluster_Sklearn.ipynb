{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse date\n",
    "dateparse = lambda x: datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "# Read CSV while parsing the dates\n",
    "opinion_df = pd.read_csv('all_opinions.csv', parse_dates=['date_filed'], date_parser=dateparse)\n",
    "\n",
    "# Copy the main df\n",
    "opinion_copy = opinion_df.copy()\n",
    "\n",
    "# Get opinions in the past 50 years\n",
    "above_1970 = opinion_copy[opinion_copy['date_filed'] > \"1970-01-01\"]\n",
    "\n",
    "# Remove Justice Douglas given how his opinions is highly unusual\n",
    "# Refer to https://www.thenation.com/article/archive/tragedy-william-o-douglas/\n",
    "above_1970_no_douglas = above_1970[above_1970['author_name'] != 'Justice Douglas']\n",
    "\n",
    "# Remove those texts with less than 3000 characters as these are recounting past opinions\n",
    "# Refer to https://www.kaggle.com/gqfiddler/scotus-opinions description of the dataset\n",
    "char_above3000_1970 = above_1970_no_douglas[above_1970_no_douglas['text'].str.len() > 3000]\n",
    "\n",
    "# Drop values that are not relevant for our analysis\n",
    "to_analyze = char_above3000_1970.drop(columns=['absolute_url', 'cluster', 'year_filed', \n",
    "                                      'scdb_id', 'date_filed', 'author_name', 'federal_cite_one',\n",
    "                                       'scdb_decision_direction', 'scdb_votes_majority', 'scdb_votes_minority'])\n",
    "# Check the new_df\n",
    "to_analyze = to_analyze.reset_index(drop=True)\n",
    "to_analyze = to_analyze[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "\n",
    "def clean(x):\n",
    "    \n",
    "    # Split the case name into array for individual capitalizing\n",
    "    case_name_array = x['case_name'].split()\n",
    "    \n",
    "    # Iterate through the words to capitalize\n",
    "    for i in range(len(case_name_array)):\n",
    "        \n",
    "        # If word not versus, capitalize it to remove later\n",
    "        if case_name_array[i] != 'v.':\n",
    "            case_name_array[i] = case_name_array[i].upper()\n",
    "    \n",
    "    # Join the case name array together\n",
    "    case_name = ' '.join(case_name_array)\n",
    "    \n",
    "    # 1. Standardizing some punctuations\n",
    "    tmp = x['text'].replace('’', \"'\")\n",
    "#     tmp = tmp.replace('“', '\"')\n",
    "#     tmp = tmp.replace('”', '\"')\n",
    "    tmp = tmp.replace('–', \"-\")\n",
    "    tmp = re.sub(r'([.]\\s+){2,10}', '', tmp)\n",
    "    tmp = tmp.replace('[', '')\n",
    "    tmp = tmp.replace(']', '')\n",
    "    \n",
    "    # 2. Remove redundant words\n",
    "    \n",
    "    # A. (i) Remove Cite as: since these are words that keep appearing at the bottom of the transcript for citation\n",
    "    tmp = re.sub(r'Cite as:(.*?)\\((\\d{4})\\)', '', tmp)\n",
    "    \n",
    "    # A. (ii) Remove Opinion of Justice given it is a demarcation of the transcript\n",
    "    tmp = re.sub(r'Opinion\\s(.*?)\\n', '', tmp)\n",
    "    \n",
    "    # B. Remove Case Name\n",
    "    tmp = re.sub(f'''{case_name}''', '', tmp)\n",
    "    \n",
    "    #B. Remove See ... (As these are citations of previous cases to be used)\n",
    "    # Three Cases\n",
    "    # i) See case and citation\n",
    "    see_pattern = re.compile(r\"See(.*?)(\\)\\.|\\)\\;|\\d\\.)\", re.DOTALL)\n",
    "    tmp = re.sub(see_pattern, '', tmp)\n",
    "    \n",
    "    # (ii) Remove quotes i.e. Herring v. New York, 422 U.S. 853, 862 (1975)\n",
    "#     tmp = re.sub(r'', '', tmp)\n",
    "\n",
    "#     print(tmp)\n",
    "    \n",
    "    # 3. Remove unicode characters in text\n",
    "    tmp = re.sub(r'[^\\x00-\\x7F]+', '', tmp)\n",
    "    \n",
    "    # 4. Remove breakline in text\n",
    "    \n",
    "    # A. Embedded in the string (-\\\\n)\n",
    "    tmp = re.sub(r'-\\n\\s{1,}', '', tmp)\n",
    "    tmp = re.sub(r'-\\n', '', tmp) \n",
    "    \n",
    "    # B. Remove long breaks\n",
    "    tmp = re.sub(r'\\n\\s+', ' ', tmp)\n",
    "    \n",
    "    # C. Remove the remaining breaklines\n",
    "    tmp = re.sub(r'\\n', ' ', tmp)\n",
    "    \n",
    "    # 5. Remove Numbers that demarcate sections of opinions\n",
    "    tmp = re.sub(r'\\s\\d{1,}\\s{2,}', ' ', tmp)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# Instantiate the set of stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist += ['u', 'state', 'court', 'id', 'amendment', 'respondent', 'appeal', 'case', 'may','could','would', 'c', 'v', 'u', 'one', 'see', 'even', 'issue', 'however', 'supra', 'clause', 'constitutional', 'jury', 'petitioner', 'j', 'requirement', 'ante', 'claim', 'standard', 'process', 'review', 'regulation', 'employee', 'judge', 'criminal', 'n', 'statutory', 'majority', 'individual', 'argument', 'benefit', 'judicial', 'policy', 'result', 'conduct', 'required', 'agency', 'school', 'officer', 'statement', 'violation', 'rather', 'particular', 'ibid', 'circumstance', 'support', 'second', 'protection', 'reasonable', 'party', 'counsel', 'basis', 'clear', 'plan', 'language', 'application', 'sentence', 'well', 'law', 'system', 'member', 'dissenting', 'principle', 'holding', 'need', 'mean', 'procedure', 'although', 'conclusion', 'based', 'private', 'app', 'defendant', 'due', 'practice', 'relief', 'respect', 'since', 'attorney', 'year', 'proceeding', 'prior', 'b', 'legislative', 'provision', 'crime', 'different', 'agreement', 'point', 'inc', 'civil', 'rule', 'provide', 'union', 'today', 'employer', 'purpose', 'way', 'legal', 'decision', 'course', 'child', 'activity', 'finding', 'offense', 'brief', 'statute', 'damage', 'history', 'hearing', 'relevant', 'simply', 'certain', 'conviction', 'ed', 'official', 'remedy', 'require', 'set', 'used', 'interpretation', 'word', 'information', 'police', 'term', 'provides', 'national', 'requires', 'class', 'intended', 'apply', 'concluded', 'death', 'interest', 'example', 'cost', 'place', 'le', 'work', 'hold', 'determination', 'exercise', 'burden', 'ii', 'indeed', 'like', 'say', 'granted', 'limitation', 'emphasis', 'consideration', 'test', 'concurring', 'find', 'take', 'concern', 'person', 'immunity', 'either', 'filed', 'discrimination', 'fact', 'give', 'iii', 'permit', 'three', 'program', 'liability', 'supp', 'meaning', 'petition', 'another', 'applied', 'search', 'report', 'limited', 'challenge', 'added', 'california', 'f', 'motion', 'exception', 'least', 'consider', 'substantial', 'intent', 'defense', 'appropriate', 'fee', 'reason', 'dissent', 'many', 'recognized', 'limit', 'injury', 'nothing', 'l']\n",
    "stopwords_set = set(stoplist)\n",
    "\n",
    "# Instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(x):\n",
    "    \n",
    "    # 1. Lower case\n",
    "    tmp = x.lower()\n",
    "    \n",
    "    # 2. Remove punctuations\n",
    "    tmp = tmp.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Tokenize the sentences\n",
    "    tokens = word_tokenize(tmp)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    no_stopwords = [word for word in tokens if word not in stopwords_set and word.isalpha()]\n",
    "    \n",
    "    # 5. Lemmatize\n",
    "    lemma_text = ' '.join([lemmatizer.lemmatize(word) for word in no_stopwords])\n",
    "    \n",
    "    return lemma_text\n",
    "\n",
    "to_analyze['cleaned_text'] = to_analyze.apply(lambda x : clean(x), axis=1)\n",
    "to_analyze['preprocessed_text'] = to_analyze['cleaned_text'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "stopList=(stop_words.ENGLISH_STOP_WORDS)\n",
    "stopList = [stop.lower() for stop in stopList]\n",
    "additional_stop_words = ['u', 'state', 'court', 'id', 'amendment', 'respondent', 'appeal', 'case', 'may','could','would', 'c', 'v', 'u', 'one', 'see', 'even', 'issue', 'however', 'supra', 'clause', 'constitutional', 'jury', 'petitioner', 'j', 'requirement', 'ante', 'claim', 'standard', 'process', 'review', 'regulation', 'employee', 'judge', 'criminal', 'n', 'statutory', 'majority', 'individual', 'argument', 'benefit', 'judicial', 'policy', 'result', 'conduct', 'required', 'agency', 'school', 'officer', 'statement', 'violation', 'rather', 'particular', 'ibid', 'circumstance', 'support', 'second', 'protection', 'reasonable', 'party', 'counsel', 'basis', 'clear', 'plan', 'language', 'application', 'sentence', 'well', 'law', 'system', 'member', 'dissenting', 'principle', 'holding', 'need', 'mean', 'procedure', 'although', 'conclusion', 'based', 'private', 'app', 'defendant', 'due', 'practice', 'relief', 'respect', 'since', 'attorney', 'year', 'proceeding', 'prior', 'b', 'legislative', 'provision', 'crime', 'different', 'agreement', 'point', 'inc', 'civil', 'rule', 'provide', 'union', 'today', 'employer', 'purpose', 'way', 'legal', 'decision', 'course', 'child', 'activity', 'finding', 'offense', 'brief', 'statute', 'damage', 'history', 'hearing', 'relevant', 'simply', 'certain', 'conviction', 'ed', 'official', 'remedy', 'require', 'set', 'used', 'interpretation', 'word', 'information', 'police', 'term', 'provides', 'national', 'requires', 'class', 'intended', 'apply', 'concluded', 'death', 'interest', 'example', 'cost', 'place', 'le', 'work', 'hold', 'determination', 'exercise', 'burden', 'ii', 'indeed', 'like', 'say', 'granted', 'limitation', 'emphasis', 'consideration', 'test', 'concurring', 'find', 'take', 'concern', 'person', 'immunity', 'either', 'filed', 'discrimination', 'fact', 'give', 'iii', 'permit', 'three', 'program', 'liability', 'supp', 'meaning', 'petition', 'another', 'applied', 'search', 'report', 'limited', 'challenge', 'added', 'california', 'f', 'motion', 'exception', 'least', 'consider', 'substantial', 'intent', 'defense', 'appropriate', 'fee', 'reason', 'dissent', 'many', 'recognized', 'limit', 'injury', 'nothing', 'l']\n",
    "stopList += additional_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning fundmental and create tokens\n",
    "def clean_text(text):\n",
    "    #tokenization\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            tokens.append(word)\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            if token not in stopList:\n",
    "                filtered_tokens.append(token)   #punctuation, stop word and NER\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning data with Lemma\n",
    "def lemma_text(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = []\n",
    "    #lemmatization and stemming\n",
    "    for token in cleaned_text:\n",
    "        stemmed_tokens.append(WordNetLemmatizer().lemmatize(token))\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = to_analyze.preprocessed_text\n",
    "lemmaList = reviews.apply(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sen token\n",
    "def sent_2_doc(document):    \n",
    "    clean_words = \" \".join([sent for sent in document])\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaDocList = [sent_2_doc(doc) for doc in lemmaList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectors\n",
    "lemma_tfidf_vec = TfidfVectorizer(tokenizer=lemma_text)\n",
    "lemma_tfidf_matrix = lemma_tfidf_vec.fit_transform(reviews)\n",
    "lemma_terms = lemma_tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = to_analyze.preprocessed_text.values\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemma_text)\n",
    "X3 = vectorizer.fit_transform(desc)\n",
    "words3= vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow\n",
    "distortions = []\n",
    "K = range(1,11)\n",
    "for i in range(2,30):\n",
    "    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "    kmeans.fit(X3)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "plt.plot(range(2,30),distortions)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('distortions')\n",
    "plt.savefig('./elbow_Kmeans.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 12 #Change this according to the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means\n",
    "lemma_km = KMeans(n_clusters=NUM_CLUSTERS, init='k-means++', max_iter=50, n_init=20)\n",
    "lemma_km.fit(lemma_tfidf_matrix)\n",
    "lemma_clusters = lemma_km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe df for only reviews. we will focus only on reviews for saving the output\n",
    "df = pd.DataFrame({'Options':lemmaDocList}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Saving to csv\n"
     ]
    }
   ],
   "source": [
    "#Storing Cluster number and labels for each comment\n",
    "comment_cluster_df = pd.DataFrame(df)\n",
    "\n",
    "order_centroids = lemma_km.cluster_centers_.argsort()[:, ::-1]\n",
    "n_words_5=5\n",
    "label_list=[]\n",
    "words_list=[]\n",
    "print(type(lemma_clusters))\n",
    "for i in range(len(lemma_clusters)):\n",
    "    topicWords = []\n",
    "    #print(lemma_clusters[i])\n",
    "    for ind in order_centroids[lemma_clusters[i], :n_words_5]:\n",
    "        #print(' %s' % clean_terms[ind]),\n",
    "        topicWords.append(lemma_terms[ind])\n",
    "    #print(topicWords)\n",
    "    label_list.append(str(lemma_clusters[i]))\n",
    "    words_list.append(str(topicWords))\n",
    "    \n",
    "comment_cluster_df['freq_words'] = label_list\n",
    "comment_cluster_df['comment_label'] = words_list\n",
    "print('Saving to csv')\n",
    "comment_cluster_df.to_csv('./clustering_output.csv', index=False)\n",
    "\n",
    "    #clean_predictedTopic_5.append(topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abortion', 'patient', 'father', 'woman', 'pregnancy']\n",
      "['election', 'candidate', 'voting', 'political', 'voter']\n",
      "['arbitration', 'labor', 'bargaining', 'contract', 'collectivebargaining']\n",
      "['religious', 'speech', 'ordinance', 'religion', 'public']\n",
      "['trial', 'evidence', 'juror', 'right', 'sentencing']\n",
      "['tax', 'income', 'taxpayer', 'commerce', 'property']\n",
      "['fourth', 'prison', 'alien', 'united', 'sentencing']\n",
      "['exemption', 'disclosure', 'foia', 'compiled', 'document']\n",
      "['vii', 'title', 'racial', 'district', 'race']\n",
      "['act', 'congress', 'federal', 'property', 'action']\n",
      "['federal', 'district', 'jurisdiction', 'action', 'habeas']\n",
      "['indian', 'tribe', 'tribal', 'reservation', 'land']\n"
     ]
    }
   ],
   "source": [
    "# Print Cluster Topics\n",
    "unique_set=set(words_list)\n",
    "#print(unique_set)\n",
    "unique_list=list(unique_set)\n",
    "for elem in unique_list:\n",
    "        print(elem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-88062695f891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# The (n_clusters+1)*10 is for inserting blank space between silhouette\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# plots of individual clusters, to demarcate them clearly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#Cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[0;32m    296\u001b[0m                         \" or shape[0]\")\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Silhoutte Approach\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X3) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    #Cluster\n",
    "    lemma_S= KMeans(n_clusters=n_clusters, init='k-means++', max_iter=200, n_init=20)\n",
    "    cluster_labels = lemma_S.fit_predict(X3)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X3, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "     # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X3, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X3[:, 0], X3[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
