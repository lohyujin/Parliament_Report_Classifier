{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# import gensim \n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from gensim import corpora\n",
    "# import datetime\n",
    "\n",
    "# pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('all_opinions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "# from pycontractions import Contractions\n",
    "import gensim\n",
    "\n",
    "# Preprocessing\n",
    "from datetime import datetime\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse date\n",
    "dateparse = lambda x: datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "# Read CSV while parsing the dates\n",
    "opinion_df = pd.read_csv('all_opinions.csv', parse_dates=['date_filed'], date_parser=dateparse)\n",
    "\n",
    "# Copy the main df\n",
    "opinion_copy = opinion_df.copy()\n",
    "\n",
    "# Get opinions in the past 50 years\n",
    "above_1970 = opinion_copy[opinion_copy['date_filed'] > \"1970-01-01\"]\n",
    "\n",
    "# Remove Justice Douglas given how his opinions is highly unusual\n",
    "# Refer to https://www.thenation.com/article/archive/tragedy-william-o-douglas/\n",
    "above_1970_no_douglas = above_1970[above_1970['author_name'] != 'Justice Douglas']\n",
    "\n",
    "# Remove those texts with less than 3000 characters as these are recounting past opinions\n",
    "# Refer to https://www.kaggle.com/gqfiddler/scotus-opinions description of the dataset\n",
    "char_above3000_1970 = above_1970_no_douglas[above_1970_no_douglas['text'].str.len() > 3000]\n",
    "\n",
    "# Drop values that are not relevant for our analysis\n",
    "to_analyze = char_above3000_1970.drop(columns=['absolute_url', 'cluster', 'year_filed', \n",
    "                                      'scdb_id', 'date_filed', 'author_name', 'federal_cite_one',\n",
    "                                       'scdb_decision_direction', 'scdb_votes_majority', 'scdb_votes_minority'])\n",
    "# Check the new_df\n",
    "to_analyze = to_analyze.reset_index(drop=True)\n",
    "to_analyze = to_analyze[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "\n",
    "def clean(x):\n",
    "    \n",
    "    # Split the case name into array for individual capitalizing\n",
    "    case_name_array = x['case_name'].split()\n",
    "    \n",
    "    # Iterate through the words to capitalize\n",
    "    for i in range(len(case_name_array)):\n",
    "        \n",
    "        # If word not versus, capitalize it to remove later\n",
    "        if case_name_array[i] != 'v.':\n",
    "            case_name_array[i] = case_name_array[i].upper()\n",
    "    \n",
    "    # Join the case name array together\n",
    "    case_name = ' '.join(case_name_array)\n",
    "    \n",
    "    # 1. Standardizing some punctuations\n",
    "    tmp = x['text'].replace('’', \"'\")\n",
    "#     tmp = tmp.replace('“', '\"')\n",
    "#     tmp = tmp.replace('”', '\"')\n",
    "    tmp = tmp.replace('–', \"-\")\n",
    "    tmp = re.sub(r'([.]\\s+){2,10}', '', tmp)\n",
    "    tmp = tmp.replace('[', '')\n",
    "    tmp = tmp.replace(']', '')\n",
    "    \n",
    "    # 2. Remove redundant words\n",
    "    \n",
    "    # A. (i) Remove Cite as: since these are words that keep appearing at the bottom of the transcript for citation\n",
    "    tmp = re.sub(r'Cite as:(.*?)\\((\\d{4})\\)', '', tmp)\n",
    "    \n",
    "    # A. (ii) Remove Opinion of Justice given it is a demarcation of the transcript\n",
    "    tmp = re.sub(r'Opinion\\s(.*?)\\n', '', tmp)\n",
    "    \n",
    "    # B. Remove Case Name\n",
    "    tmp = re.sub(f'''{case_name}''', '', tmp)\n",
    "    \n",
    "    #B. Remove See ... (As these are citations of previous cases to be used)\n",
    "    # Three Cases\n",
    "    # i) See case and citation\n",
    "    see_pattern = re.compile(r\"See(.*?)(\\)\\.|\\)\\;|\\d\\.)\", re.DOTALL)\n",
    "    tmp = re.sub(see_pattern, '', tmp)\n",
    "    \n",
    "    # (ii) Remove quotes i.e. Herring v. New York, 422 U.S. 853, 862 (1975)\n",
    "#     tmp = re.sub(r'', '', tmp)\n",
    "\n",
    "#     print(tmp)\n",
    "    \n",
    "    # 3. Remove unicode characters in text\n",
    "    tmp = re.sub(r'[^\\x00-\\x7F]+', '', tmp)\n",
    "    \n",
    "    # 4. Remove breakline in text\n",
    "    \n",
    "    # A. Embedded in the string (-\\\\n)\n",
    "    tmp = re.sub(r'-\\n\\s{1,}', '', tmp)\n",
    "    tmp = re.sub(r'-\\n', '', tmp) \n",
    "    \n",
    "    # B. Remove long breaks\n",
    "    tmp = re.sub(r'\\n\\s+', ' ', tmp)\n",
    "    \n",
    "    # C. Remove the remaining breaklines\n",
    "    tmp = re.sub(r'\\n', ' ', tmp)\n",
    "    \n",
    "    # 5. Remove Numbers that demarcate sections of opinions\n",
    "    tmp = re.sub(r'\\s\\d{1,}\\s{2,}', ' ', tmp)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# Instantiate the set of stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist += ['u', 'state', 'court', 'id', 'amendment', 'federal', 'respondent', 'appeal', 'case', 'may','could','would', 'c', 'v', 'u', 'one', 'see', 'even', 'issue', 'however', 'supra', 'clause', 'constitutional', 'jury', 'petitioner', 'j', 'requirement', 'ante', 'claim', 'standard', 'process', 'review', 'regulation', 'employee', 'judge', 'criminal', 'n', 'statutory', 'majority', 'individual', 'argument', 'benefit', 'judicial', 'policy', 'result', 'conduct', 'required', 'agency', 'school', 'officer', 'statement', 'violation', 'rather', 'particular', 'ibid', 'circumstance', 'support', 'second', 'protection', 'reasonable', 'party', 'counsel', 'basis', 'clear', 'plan', 'language', 'application', 'sentence', 'well', 'law', 'system', 'member', 'dissenting', 'principle', 'holding', 'need', 'mean', 'procedure', 'although', 'conclusion', 'based', 'private', 'app', 'defendant', 'due', 'practice', 'relief', 'respect', 'since', 'attorney', 'year', 'proceeding', 'prior', 'b', 'legislative', 'provision', 'crime', 'different', 'agreement', 'point', 'inc', 'civil', 'rule', 'provide', 'union', 'today', 'employer', 'purpose', 'way', 'legal', 'decision', 'course', 'child', 'activity', 'finding', 'offense', 'brief', 'statute', 'damage', 'history', 'hearing', 'relevant', 'simply', 'certain', 'conviction', 'ed', 'official', 'remedy', 'require', 'set', 'used', 'interpretation', 'word', 'information', 'police', 'term', 'provides', 'national', 'requires', 'class', 'intended', 'apply', 'concluded', 'death', 'interest', 'example', 'cost', 'place', 'le', 'work', 'hold', 'determination', 'exercise', 'burden', 'ii', 'indeed', 'like', 'say', 'granted', 'limitation', 'emphasis', 'consideration', 'test', 'concurring', 'find', 'take', 'concern', 'person', 'immunity', 'either', 'filed', 'discrimination', 'fact', 'give', 'iii', 'permit', 'three', 'program', 'liability', 'supp', 'meaning', 'petition', 'another', 'applied', 'search', 'report', 'limited', 'challenge', 'added', 'california', 'f', 'motion', 'exception', 'least', 'consider', 'substantial', 'intent', 'defense', 'appropriate', 'fee', 'reason', 'dissent', 'many', 'recognized', 'limit', 'injury', 'nothing', 'l']\n",
    "stopwords_set = set(stoplist)\n",
    "\n",
    "# Instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(x):\n",
    "    \n",
    "    # 1. Lower case\n",
    "    tmp = x.lower()\n",
    "    \n",
    "    # 2. Remove punctuations\n",
    "    tmp = tmp.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Tokenize the sentences\n",
    "    tokens = word_tokenize(tmp)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    no_stopwords = [word for word in tokens if word not in stopwords_set and word.isalpha()]\n",
    "    \n",
    "    # 5. Lemmatize\n",
    "    lemma_text = ' '.join([lemmatizer.lemmatize(word) for word in no_stopwords])\n",
    "    \n",
    "    return lemma_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyze['cleaned_text'] = to_analyze.apply(lambda x : clean(x), axis=1)\n",
    "to_analyze['preprocessed_text'] = to_analyze['cleaned_text'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>right basic democracy right participate electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>adhere view court buckley valeo u per curiam d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nearly year ago buckley valeo u per curiam con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>federal u authorizes freeze indicted defendant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>facing serious charge brought united state lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>early morning hour november thomas joe millere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>missouri imposes uniform statewide use tax goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>martinez ryan u considered right prisoner rais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>federal court principal forum asserting challe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>indian tribe generally entitled suit considere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    right basic democracy right participate electi...\n",
       "1    adhere view court buckley valeo u per curiam d...\n",
       "2    nearly year ago buckley valeo u per curiam con...\n",
       "3    federal u authorizes freeze indicted defendant...\n",
       "4    facing serious charge brought united state lit...\n",
       "..                                                 ...\n",
       "495  early morning hour november thomas joe millere...\n",
       "496  missouri imposes uniform statewide use tax goo...\n",
       "497  martinez ryan u considered right prisoner rais...\n",
       "498  federal court principal forum asserting challe...\n",
       "499  indian tribe generally entitled suit considere...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem = pd.DataFrame({'text' : []})\n",
    "df_lem['text'] = to_analyze['preprocessed_text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>right basic democracy right participate electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>adhere view court valeo u per curiam denigrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nearly ago valeo u per curiam considered const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>federal authorizes freeze indicted defendant a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>facing serious charge brought united state lit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  right basic democracy right participate electi...\n",
       "1  adhere view court valeo u per curiam denigrate...\n",
       "2  nearly ago valeo u per curiam considered const...\n",
       "3  federal authorizes freeze indicted defendant a...\n",
       "4  facing serious charge brought united state lit..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def no_name(text):\n",
    "    nlp =  spacy.load('en_core_web_sm')\n",
    "    doc_no_name_entities = []\n",
    "    \n",
    "    #Convert into spacy class type to remove name entity\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #Identify name entity in document\n",
    "    ents = [e.text for e in doc.ents]\n",
    "    \n",
    "    #Selection of words that do not contain name entity\n",
    "    for item in doc:\n",
    "        #Identify name entity in document\n",
    "        if item.text in ents:\n",
    "            pass\n",
    "        else:\n",
    "            doc_no_name_entities.append(item.text)\n",
    "    \n",
    "    #Merge word tokens into text\n",
    "    doc_no_name_entities = \" \".join(doc_no_name_entities)\n",
    "    \n",
    "    return doc_no_name_entities\n",
    "\n",
    "df_lem['text'] = df_lem['text'].apply(lambda x: no_name(x))\n",
    "df_lem['text'] = df_lem['text'].apply(lambda x: [word for word in x.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop all Nan row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop all Nan\n",
    "# df.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples=df.copy(deep=True)\n",
    "# samples = samples.sort_values(by='date_filed', ascending=False)\n",
    "# sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove special characters (\\n, punctuations, numbers)\n",
    "3. Lemmatisation on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer= WordNetLemmatizer()\n",
    "# start = datetime.datetime.now()\n",
    "\n",
    "# stop_list = stopwords.words('english')\n",
    "\n",
    "# sample['text'] = sample['text'].apply(lambda x: [lemmatizer.lemmatize(re.sub('[^A-Za-z]+', '', word.lower())) for word in x.split()])\n",
    "# print(\"Time taken: \", datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "\n",
    "# doc_lem = sample['text'].values.tolist()\n",
    "\n",
    "# a = []\n",
    "\n",
    "# for i in doc_lem:\n",
    "#     a += i\n",
    "\n",
    "# # Pass the split_it list to instance of Counter class. \n",
    "# Counter = Counter(a) \n",
    "  \n",
    "# # most_common() produces k frequently encountered \n",
    "# # input values and their respective counts. \n",
    "# most_occur = Counter.most_common(200) \n",
    "\n",
    "# def Convert(tup, di): \n",
    "#     for a, b in tup: \n",
    "#         di.setdefault(a, []).append(b) \n",
    "#     return di \n",
    "\n",
    "# dictionary = {}\n",
    "# words = Convert(most_occur, dictionary)\n",
    "# print(words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Removing stopwords and splitting into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # law corpus\n",
    "# # stop_list += ['section', 'relevant', 'period', 'submission', 'grant', 'appropriate', 'behalf', 'convention', 'assessment', 'process', 'commissioner', 'previous', 'enable', 'transfer', 'justify', 'specific', 'comment', 'contact', 'final', 'error', 'confirm', 'interpretation', 'discrimination', 'create', 'subsequent', 'sexual', 'compensation', 'capable', 'objective', 'attach', 'category', 'obvious', 'feature', 'considerable', 'subsequently', 'instruction', 'confer', 'summarise', 'fee', 'link', 'investigate', 'overall', 'resolution', 'enforce', 'initial', 'recovery', 'sole', 'inference', 'maintenance', 'regime', 'priority', 'residential', 'obviously', 'respond', 'couple', 'maximum', 'welfare', 'potentially', 'temporary', 'attribute', 'participate', 'correspondence', 'whereas', 'promote', 'solely', 'deduction', 'technical', 'finally', 'violation', 'exceed', 'primarily', 'brief', 'eventually', 'recoverable', 'compatible', 'annual', 'disproportionate', 'communicate', 'military', 'hence', 'core', 'consumer', 'facilitate', 'assure', 'reverse', 'adjustment', 'restrain', 'environment', 'formulate', 'briefly', 'institution', 'intervention', 'foundation', 'identical', 'evidential', 'legality', 'enhance', 'trigger', 'specification', 'regulatory', 'presumably', 'practitioner', 'appreciation', 'presume', 'index', 'constructive', 'analogy', 'alternatively', 'appendix', 'unaware', 'clarity', 'accurately', 'trace', 'consequently', 'adapt', 'via', 'conversion', 'reluctant', 'precedent', 'integrity', 'appropriately', 'revision', 'equate', 'occurrence', 'attain', 'inhibit', 'co-operation', 'contradict', 'pursuit', 'isolate', 'explicit', 'co-operate', 'consistency', 'classic', 'displace', 'constrain', 'dominant', 'constant', 'administrator', 'compatibility', 'flexible', 'conceive', 'evidently', 'theme', 'incentive', 'uniform', 'commentary', 'mode', 'succession', 'temporarily', 'detective', 'computation', 'regulator', 'volunteer', 'periodic', 'mature', 'controversy', 'constraint', 'mutually', 'sphere', 'evolve', 'rationality', 'journal', 'unaccompanied', 'exploit', 'tradition', 'concentration', 'retainer', 'transmission', 'survivor', 'denote', 'style', 'institutional', 'sustainable', 'enormous', 'trend', 'norm', 'cooperate', 'demonstrably', 'constituent', 'professionally', 'variable', 'abandonment', 'interpretative', 'radically', 'appreciable', 'negate', 'chart', 'reversal', 'lecture', 'estimation', 'adaptation', 'traditionally', 'random', 'medically', 'erroneously', 'linkage', 'philosophy', 'proportionately', 'exporter', 'predictable', 'legislator', 'proportional', 'convening', 'vision', 'emphasize', 'periodically', 'founder', 'selective', 'irreversible', 'subsidy', 'crucially', 'relaxation', 'instability', 'ethics', 'revelation', 'co-ordination', 'publisher', 'phenomenon', 'incapacitate', 'nuclear', 'obtainable', 'compilation', 'philosophical', 'immature', 'liberation', 'demonstrator', 'statistically', 'dynamics', 'transited', 'constitutive', 'subordination', 'thesis', 'survival', 'neutralise', 'variability', 'lecturer', 'economist', 'traceable', 'incoherent', 'resourceful', 'constitutionally', 'automate', 'unethical', 'randomness', 'constituency', 'institutionally', 'creative', 'readjust', 'expansive', 'coherently', 'undeniable', 'computational', 'unconventional', 'academy', 'redraft', 'recreate', 'constancy', 'attainable', 'conformable', 'analytically', 'unparalleled', 'restrictively', 'funder', 'expertly', 'minimize', 'chemically', 'insightful', 'unattached', 'ideological', 'unpublished', 'inaccessible', 'transferable', 'contextualize', 'schematically', 'institutionalise', 'evidence', 'authority', 'submit', 'conclusion', 'approach', 'conduct', 'identify', 'policy', 'individual', 'indicate', 'sufficient', 'significant', 'investigation', 'release', 'commission', 'community', 'assume', 'specify', 'purchase', 'scope', 'response', 'site', 'achieve', 'aspect', 'income', 'register', 'assist', 'role', 'analysis', 'criterion', 'fundamental', 'majority', 'procedural', 'impact', 'apparent', 'outcome', 'incident', 'cease', 'inconsistent', 'interpret', 'commence', 'inspector', 'specifically', 'data', 'normally', 'task', 'significance', 'notwithstanding', 'adequate', 'method', 'indication', 'sustain', 'consist', 'framework', 'presumption', 'dispose', 'restraint', 'reside', 'implication', 'series', 'contribute', 'variation', 'prohibit', 'irrelevant', 'proportionate', 'inevitably', 'occupation', 'exclusion', 'inappropriate', 'distribution', 'validity', 'disposal', 'beneficial', 'attributable', 'adult', 'extract', 'consult', 'hypothetical', 'commencement', 'imposition', 'margin', 'environmental', 'legislature', 'perceive', 'emphasis', 'approximately', 'select', 'assignment', 'assign', 'mechanism', 'transitional', 'automatically', 'technology', 'occupier', 'ministry', 'selection', 'adequately', 'indefinite', 'inadequate', 'retention', 'exclusively', 'undergo', 'survey', 'aggregate', 'construct', 'label', 'incapable', 'distribute', 'reaction', 'assistant', 'survive', 'commitment', 'locate', 'highlight', 'device', 'image', 'supplement', 'conference', 'attachment', 'levy', 'manual', 'evaluation', 'legislate', 'logical', 'evident', 'occupational', 'acknowledgement', 'flexibility', 'evaluate', 'so-called', 'restoration', 'complexity', 'portion', 'overlap', 'technique', 'logically', 'motivate', 'furthermore', 'eventual', 'predict', 'economically', 'implicate', 'successor', 'region', 'abstract', 'scenario', 'invariably', 'tape', 'authoritative', 'ratio', 'conformity', 'interaction', 'widespread', 'indicative', 'precision', 'tension', 'justifiable', 'extraction', 'principled', 'currency', 'collapse', 'technically', 'utilise', 'investor', 'focusing', 'invalidity', 'deduce', 'illogical', 'comprehensively', 'persistent', 'maturity', 'finalise', 'odds', 'unambiguous', 'ambiguous', 'compile', 'append', 'incidence', 'insufficiently', 'instructive', 'goal', 'transform', 'coincidence', 'decade', 'definitely', 'approximate', 'diversity', 'domain', 'unaffected', 'liberal', 'precedence', 'inaccuracy', 'prioritisation', 'passive', 'complement', 'reassessment', 'fluctuate', 'reformulate', 'disproportionately', 'attainment', 'deviation', 'eventuality', 'intrinsically', 'marginally', 'visual', 'reluctantly', 'persistently', 'abnormal', 'domesticate', 'theoretically', 'textual', 'abnormally', 'uniformity', 'convincingly', 'financier', 'validation', 'reformulation', 'sexuality', 'elimination', 'structurally', 'designer', 'projection', 'identically', 'restructuring', 'uniquely', 'visibility', 'incidentally', 'itemise', 'insecure', 'analytical', 'coordination', 'technological', 'dynamic', 'debatable', 'aggregation', 'preliminaries', 'securely', 'editorial', 'unaltered', 'innovative', 'unconvinced', 'misinterpretation', 'enormity', 'visualise', 'responsive', 'sequentially', 'misinterpreting', 'cyclical', 'reassess', 'intensely', 'methodical', 'predominate', 'energetically', 'psychologically', 'analyze', 'reoccur', 'rigidity', 'dominance', 'domination', 'hypothesise', 'evolutionary', 'unstructured', 'unanticipated', 'migrate', 'finalize', 'symbolic', 'coincident', 'regionally', 'uniqueness', 'illogically', 'utilization', 'indiscretion', 'transference', 'individualist', 'methodological', 'issue', 'circumstance', 'regulation', 'involve', 'legal', 'obtain', 'document', 'available', 'context', 'area', 'immigration', 'consequence', 'constitute', 'financial', 'ensure', 'medical', 'assess', 'construction', 'occur', 'amend', 'remove', 'alternative', 'function', 'amendment', 'aware', 'mental', 'consistent', 'demonstrate', 'pursue', 'cite', 'file', 'resolve', 'distinction', 'acknowledge', 'previously', 'restriction', 'option', 'status', 'estate', 'professional', 'conflict', 'sufficiently', 'imply', 'creditor', 'concept', 'positive', 'underlie', 'confine', 'legislative', 'apparently', 'decline', 'identity', 'partner', 'route', 'valid', 'somewhat', 'incompatible', 'finance', 'sex', 'initially', 'pose', 'substitute', 'alter', 'implement', 'investment', 'legally', 'ignore', 'exclusive', 'contractor', 'consultant', 'stress', 'corporate', 'guideline', 'expose', 'documentation', 'transport', 'crucial', 'prohibition', 'protocol', 'formulation', 'analyse', 'labour', 'distinct', 'ultimate', 'beneficiary', 'ethnic', 'establishment', 'discriminate', 'negative', 'reinforce', 'notion', 'insert', 'violate', 'author', 'entity', 'participation', 'compensate', 'inevitable', 'minority', 'definitive', 'parallel', 'duration', 'incline', 'psychological', 'display', 'diminish', 'inconsistency', 'intelligence', 'alteration', 'irrational', 'expertise', 'consequent', 'validly', 'reliability', 'subordinate', 'theory', 'bias', 'export', 'conclusive', 'rejection', 'exhibit', 'perception', 'supplementary', 'assembly', 'inspect', 'cycle', 'adjacent', 'psychologist', 'explicitly', 'categorise', 'gender', 'exposure', 'similarity', 'encounter', 'unjustified', 'motivation', 'illegality', 'federal', 'positively', 'substitution', 'channel', 'stability', 'depression', 'insight', 'definite', 'accessible', 'functional', 'persist', 'bulk', 'distort', 'occupant', 'compute', 'predominant', 'ministerial', 'conceivable', 'unfounded', 'rigid', 'restructure', 'theoretical', 'illegally', 'terminal', 'inconceivable', 'psychology', 'maximise', 'relocate', 'demonstration', 'culture', 'insertion', 'migrant', 'orientation', 'transmit', 'validate', 'marginal', 'conclusively', 'constantly', 'anticipation', 'economy', 'indicator', 'reliant', 'detection', 'relax', 'inflexible', 'enhancement', 'format', 'predominantly', 'intelligent', 'prediction', 'primacy', 'consultancy', 'signify', 'justifiably', 'exclusionary', 'correspondingly', 'ethnicity', 'distinctly', 'contradiction', 'occupancy', 'misinterpret', 'unify', 'distortion', 'expansion', 'integration', 'underestimate', 'equivalence', 'variant', 'specificity', 'unambiguously', 'annually', 'conceptually', 'predictability', 'advocacy', 'achievable', 'contributor', 'complementary', 'stressful','abstraction', 'differentiation', 'subsidise', 'persistence', 'unprincipled', 'unbiased', 'reactivate', 'prohibitive', 'unconstrained', 'undefined', 'co-ordinate', 'subordinates', 'mediate', 'classical', 'challenger', 'distributive', 'unprecedented', 'rigidly', 'emergence', 'facilitator', 're-evaluation', 'commodity', 'liberally', 'coordinate', 'negatively', 'unregulated', 'max', 'reactive', 'derivation', 'transitory', 'intelligently', 'professionalism', 'devotion', 'affective', 'normality', 'paralleled', 'consultative', 'inflexibility', 'bulky', 'creator', 'stylise', 'visually', 'mentality', 'exhibition', 'unalterable', 'functionally', 'accompaniment', 'diversification', 'reactor', 'ideology', 'innovator', 'creativity', 'reversible', 'unmodified', 'inconstancy', 'adaptability', 'overestimate', 'unresponsive', 'normalisation', 'proportionally', 'paragraph', 'seek', 'principle', 'respondent', 'rely', 'requirement', 'clause', 'schedule', 'proceed', 'challenge', 'commit', 'legislation', 'similar', 'secure', 'sum', 'injury', 'contrary', 'prior', 'domestic', 'define', 'definition', 'acquire', 'summary', 'primary', 'panel', 'recover', 'expert', 'element', 'physical', 'assumption', 'administrative', 'design', 'despite', 'instance', 'consent', 'reliance', 'quote', 'economic', 'resident', 'residence', 'occupy', 'capacity', 'minimum', 'registration', 'vary', 'publish', 'derive', 'albeit', 'nevertheless', 'corporation', 'terminate', 'structure', 'involvement', 'communication', 'network', 'consultation', 'relevance', 'license', 'precisely', 'estimate', 'ultimately', 'appreciate', 'revise', 'debate', 'accompany', 'contrast', 'inherent', 'purchaser', 'subsidiary', 'project', 'internal', 'invoke', 'reveal', 'team', 'emerge', 'insufficient', 'allocate', 'infer', 'computer', 'monitor', 'proportion', 'complex', 'intervene', 'consistently', 'denial', 'target', 'whereby', 'medium', 'modification', 'anticipate', 'precede', 'accurate', 'generate', 'external', 'constitutional', 'institute', 'motive', 'implementation', 'availability', 'constitution', 'clarify', 'mutual', 'citation', 'objectively', 'ongoing', 'implicit', 'initiate', 'straightforward', 'conventional', 'restore', 'grade', 'prime', 'virtually', 'traditional', 'confirmation', 'formula', 'expand', 'overseas', 'incompatibility', 'intermediate', 'undertaking', 'physically', 'fundamentally', 'compensatory', 'likewise', 'eliminate', 'colleague', 'volume', 'participant', 'voluntarily', 'sector', 'interval', 'minimise', 'successive', 'clarification', 'investigator', 'statistics', 'capability', 'sexually', 'offset', 'conform', 'financially', 'invalidate', 'cultural', 'illustration', 'hypothesis', 'inadequacy', 'ambiguity', 'minimal', 'transit', 'isolation', 'utility', 'react', 'investigative', 'distributor', 'diminution', 'coincide', 'global', 'editor', 'conceivably', 'relocation', 'energy', 'cooperation', 'promoter', 'intensive', 'coherent', 'intense', 'integrate', 'assemble', 'differentiate', 'distinctive', 'generation', 'mediation', 'indefinitely', 'rationally', 'co-operative', 'finality', 'unlicensed', 'accumulate', 'intensity', 'refine', 'internally', 'concurrently', 'immigrant', 'variance', 'methodology', 'reconstruction', 'converse', 'inappropriately', 'intrinsic', 'sufficiency', 'illustrative', 'uncontroversial', 'evolution', 'evaluative', 'diverse', 'conceptual', 'unpredictable', 'finite', 'innovation', 'imprecise', 'qualitative', 'tense', 'reconstruct', 'dominate', 'demonstrable', 'rationalisation', 'underlay', 'dramatically', 'unpredictability', 'reliably', 'conception', 'cooperative', 'transportation', 'contextual', 'objectivity', 'brevity', 'inhibition', 'accumulation', 'accessibility', 'coherence', 'appreciably', 'rationalise', 'unsustainable', 'uniformly', 'collapsible', 'approximation', 'virtual', 'intensify', 'infinitely', 'inconclusive', 'environmentally', 'unaided', 'summarize', 'selectively', 'reinforcement', 'definable', 'minimally', 'disposable', 'researcher', 'successively', 'analyst', 'redefine', 'detectable', 'predictably', 'hypothetically', 'utilize', 'globally', 'empirical', 'removable', 'revolution', 'displacement', 'unintelligent', 'ethic', 'imagery', 'innovate', 'adulthood', 'stabilise', 'immaturity', 'unscheduled', 'predominance', 'conceptualise', 'intensification', 'briefing', 'manually', 'passivity', 'discretely', 'simulation', 'coordinator', 'itemisation', 'co-ordinator', 'reactivation', 'unrestrained', 'participatory', 'unidentifiable', 'require', 'proceeding', 'benefit', 'contract', 'conclude', 'establish', 'security', 'procedure', 'impose', 'factor', 'scheme', 'reject', 'access', 'affect', 'draft', 'civil', 'discretion', 'exclude', 'fund', 'undertake', 'principal', 'maintain', 'potential', 'accommodation', 'deny', 'advocate', 'licence', 'prospect', 'assistance', 'credit', 'vehicle', 'guarantee', 'enforcement', 'instruct', 'removal', 'focus', 'equipment', 'retain', 'range', 'item', 'preliminary', 'normal', 'restrict', 'contribution', 'code', 'incorporate', 'emphasise', 'source', 'suspend', 'revenue', 'resource', 'facility', 'justification', 'chapter', 'administration', 'assurance', 'equivalent', 'precise', 'aid', 'version', 'publication', 'thereby', 'comprise', 'acquisition', 'significantly', 'injure', 'job', 'illegal', 'major', 'component', 'termination', 'identification', 'partnership', 'minor', 'suspension', 'allocation', 'discretionary', 'text', 'prospective', 'location', 'nonetheless', 'regulate', 'modify', 'abandon', 'similarly', 'bond', 'correspond', 'illustrate', 'attitude', 'accommodate', 'percentage', 'comprehensive', 'research', 'concentrate', 'convert', 'reliable', 'input', 'voluntary', 'inspection', 'plus', 'inherently', 'regional', 'rational', 'convince', 'concurrent', 'automatic', 'principally', 'detect', 'induce', 'stable', 'academic', 'edition', 'restrictive', 'topic', 'considerably', 'analogous', 'quotation', 'arbitrary', 'devote', 'shift', 'creation', 'incorporation', 'vol', 'adjust', 'sequence', 'attribution', 'erroneous', 'periodical', 'identifiable', 'logic', 'chemical', 'strategy', 'phase', 'invest', 'discrete', 'odd', 'adequacy', 'implicitly', 'promotion', 'compound', 'neutral', 'individually', 'integral', 'mentally', 'controversial', 'structural', 'accuracy', 'derivative', 'perspective', 'consensus', 'inaccurate', 'awareness', 'dimension', 'convene', 'forthcoming', 'consumption', 'ignorance', 'visible', 'output', 'appropriateness', 'reluctance', 'consume', 'contemporary', 'unique', 'strategic', 'achievement', 'radical', 'initiative', 'unspecified', 'equip', 'insignificant', 'exploitation', 'prioritise', 'depress', 'commentator', 'categorisation', 'paradigm', 'unresolved', 'unreliable', 'transition', 'interact', 'manipulate', 'infrastructure', 'parameter', 'assessable', 'unavailable', 'edit', 'ignorant', 'layer', 'dissimilar', 'contradictory', 'unrestricted', 'globe', 'hierarchy', 'refinement', 'erosion', 'neutrality', 'conventionally', 'dramatic', 'inclination', 'induction', 'statistical', 'equation', 'domestically', 'unstable', 'manipulation', 'deviate', 'schematic', 'inadequately', 'ethical', 'alternate', 'federation', 'fluctuation', 'erode', 'conversely', 'emphatically', 'optional', 'invariable', 'hierarchical', 'administratively', 'enormously', 'arbitrarily', 'arbitrariness', 'emphatic', 'economical', 're-evaluate', 'inevitability', 'detector', 'migration', 'irrelevance', 'manipulative', 'unconstitutional', 'infinite', 'instructor', 'utilisation', 'symbol', 'economics', 'orientate', 'externally', 'convertible', 'transformation', 'visibly', 'reinvest', 'initiation', 'coincidental', 'redistribution', 'academia', 'liberate', 'energetic', 'authorship', 'sustenance', 'facilitation', 'sustainability', 'orient', 'percent', 'randomly', 'assuredly', 'symbolise', 'sequential', 'academically', 'redistribute', 'individuality', 'drama', 'emergent', 'maximize', 'statistic', 'indistinct', 'unassessed', 'dimensional', 'prioritises', 'conformation', 'readjustment', 'contextualise', 'revolutionary', 'controversially']\n",
    "\n",
    "# # eyeball + top 100 common\n",
    "# stop_list += ['court', 'law', 'claim', 'state', 'made', 'case', 'act', 'city', 'right', 'rights', 'defendant', 'may', \n",
    "#                 'the', 'of', '', 'to', 'and', 'a', 'in', 'that', 'it', 'is', 'by', 'wa', 'be', 'for', 'or', 'not', 'court', 'state', 'which', 'this', 'on', 'v', 'from', 'with', 'an', 'case', 'upon', 'such', 'act', 'u', 'any', 'are', 'no', 'at', 'were', 'but', 'have', 'had', 'under', 'been', 'law', 'company', 'united', 'we', 'may', 'his', 'right', 'if', 'other', 'made', 'there', 'all', 'he', 'ha', 'said', 'would', 'one', 'question', 'so', 'statute', 'their', 'they', 'co', 'property', 'power', 'land', 'shall', 'judgment', 'tax', 'within', 'fact', 'defendant', 'order', 'only', 'against', 'time', 'when', 'should', 'contract', 'plaintiff', 'these', 'same', 'them', 'claim', 'appeal', 'purpose', 'district', 'without', 'congress', 'action', 'commission', 'new', 'provision', 'jurisdiction', 'held', 'whether', 'than', 'must', 'section', 'before', 'after', 'part', 'decision', 'suit', 'could', 'corporation', 'railroad', 'will', 'rate', 'those', 'did', 'public', 'between', 'general', 'government', 'subject', 'commerce', 'evidence', 'business', 'year', 'because', 'party', 'opinion', 'circuit', 'federal', 'who', 'where', 'into', 'person', 'effect', 'also', 'authority', 'doe', 'error', 'make', 'proceeding', 'interest', 'rule', 'amount', 'use', 'city', 'him', 'constitution', 'here', 'bank', 'do', 'first', 'more', 'being', 'out', 'interstate', 'petitioner', 'what', 'decree', 'two', 'bill', 'ground', 'value', 'can', 'sale', 'might', 'then', 'some', 'therefore', 'term', 'line', 'cannot', 'matter', 'c', 'over', 'cause', 'supreme', 'each', 'stat', 'justice', 'given', 'title', 'duty', 'service', 'present', 'further', 'provided', 'condition', 'paid', 'carrier', 'reason', 'taken', 'found', 'trial', 'brought', 'necessary', 'payment', 'board', 'mr', 'record', 'stock', 'construction', 'view', 'patent', 'thus']\n",
    "\n",
    "# stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample['text']= sample['text'].apply(lambda x: [word.lower() for word in x.split() if word.lower() not in stop_list])\n",
    "# sample['text'] = sample['text'].apply(lambda x: list(set(x) - stop_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = sample['text'].values.tolist()\n",
    "# bi = gensim.models.phrases.Phrases(doc, min_count=3, threshold=10)\n",
    "# tri = gensim.models.phrases.Phrases(bi[doc], min_count=3, threshold=10)\n",
    "# sample['text'] = tri[bi[doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem=sample.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1) Create dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.1) Create list from dataframe <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df lemmatise only\n",
    "doc_lem= df_lem['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.2) Create dictionary <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lem=corpora.Dictionary(doc_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id2=dict_lem.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.3) TF <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lem= [dict_lem.doc2bow(doc) for doc in doc_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA model with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) LDA with term freq (tf)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:00:02.663871\n",
      "\n",
      "Coherence Score LDA-lem:  0.2954909827519194\n",
      "[(0,\n",
      "  '0.006*\"right\" + 0.006*\"court\" + 0.006*\"u\" + 0.005*\"federal\" + 0.005*\"state\" '\n",
      "  '+ 0.005*\"act\" + 0.005*\"appeal\" + 0.005*\"district\" + 0.004*\"question\" + '\n",
      "  '0.004*\"whether\"'),\n",
      " (1,\n",
      "  '0.009*\"state\" + 0.006*\"united\" + 0.006*\"federal\" + 0.005*\"court\" + '\n",
      "  '0.005*\"right\" + 0.005*\"act\" + 0.005*\"u\" + 0.004*\"whether\" + '\n",
      "  '0.004*\"district\" + 0.004*\"public\"'),\n",
      " (2,\n",
      "  '0.007*\"act\" + 0.007*\"u\" + 0.007*\"federal\" + 0.006*\"state\" + 0.005*\"also\" + '\n",
      "  '0.005*\"government\" + 0.005*\"court\" + 0.004*\"district\" + 0.004*\"time\" + '\n",
      "  '0.004*\"must\"'),\n",
      " (3,\n",
      "  '0.008*\"u\" + 0.008*\"state\" + 0.007*\"act\" + 0.006*\"court\" + 0.006*\"federal\" + '\n",
      "  '0.005*\"united\" + 0.004*\"action\" + 0.004*\"appeal\" + 0.004*\"question\" + '\n",
      "  '0.004*\"also\"'),\n",
      " (4,\n",
      "  '0.008*\"u\" + 0.008*\"state\" + 0.007*\"act\" + 0.005*\"united\" + 0.005*\"court\" + '\n",
      "  '0.005*\"right\" + 0.005*\"whether\" + 0.004*\"federal\" + 0.004*\"district\" + '\n",
      "  '0.004*\"government\"'),\n",
      " (5,\n",
      "  '0.011*\"district\" + 0.007*\"u\" + 0.007*\"state\" + 0.006*\"court\" + '\n",
      "  '0.005*\"right\" + 0.004*\"federal\" + 0.004*\"whether\" + 0.004*\"government\" + '\n",
      "  '0.004*\"act\" + 0.004*\"appeal\"')]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lda_reviews_lem= gensim.models.ldamodel.LdaModel(corpus=vec_lem, id2word=dict_lem, num_topics=6, random_state=100, alpha=0.001)\n",
    "print(\"Time taken: \", datetime.now()-start)\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence score for lemmatising -tf only\n",
    "coherence_model_lda_lem = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "coherence_lda_lem = coherence_model_lda_lem.get_coherence()\n",
    "print('\\nCoherence Score LDA-lem: ', coherence_lda_lem)\n",
    "\n",
    "from pprint import pprint\n",
    "optimal_model = lda_reviews_lem\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_lem=lda_reviews_lem.show_topics(12,15)\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     print(topics_lem[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign topic back to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem['topic'] = [optimal_model.get_document_topics(dict_lem.doc2bow(doc)) for doc in doc_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[right, basic, democracy, right, participate, ...</td>\n",
       "      <td>[(2, 0.99999887)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[adhere, view, court, valeo, u, per, curiam, d...</td>\n",
       "      <td>[(2, 0.99999046)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[nearly, ago, valeo, u, per, curiam, considere...</td>\n",
       "      <td>[(2, 0.9999986)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[federal, authorizes, freeze, indicted, defend...</td>\n",
       "      <td>[(0, 0.020580342), (1, 0.63426846), (2, 0.0615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[facing, serious, charge, brought, united, sta...</td>\n",
       "      <td>[(0, 0.019055143), (1, 0.6541642), (2, 0.24891...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [right, basic, democracy, right, participate, ...   \n",
       "1  [adhere, view, court, valeo, u, per, curiam, d...   \n",
       "2  [nearly, ago, valeo, u, per, curiam, considere...   \n",
       "3  [federal, authorizes, freeze, indicted, defend...   \n",
       "4  [facing, serious, charge, brought, united, sta...   \n",
       "\n",
       "                                               topic  \n",
       "0                                  [(2, 0.99999887)]  \n",
       "1                                  [(2, 0.99999046)]  \n",
       "2                                   [(2, 0.9999986)]  \n",
       "3  [(0, 0.020580342), (1, 0.63426846), (2, 0.0615...  \n",
       "4  [(0, 0.019055143), (1, 0.6541642), (2, 0.24891...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem.head(5)\n",
    "# possible topic of 4 and 5, adding probability to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a model to disk, or reload a pre-trained model\n",
    "import pickle\n",
    "\n",
    "ldapickle = open('bryclean_lda', \"wb\")\n",
    "pickle.dump(optimal_model, ldapickle)\n",
    "ldapickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----END----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Optimal Number of the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Model with tf vectors - DONT RUN THIS<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run. In this case we are going to  k_max=10.\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "model_list = []\n",
    "coherence_values = []\n",
    "model_topics = []\n",
    "\n",
    "for num_topics in range(2, 14, 2):\n",
    "    #sg_lda_x = gensim.models.ldamodel.LdaModel(corpus=sg_vecs, id2word=sg_dictionary, num_topics=num_topics)\n",
    "    lda_reviews_lem= gensim.models.ldamodel.LdaModel(corpus=vec_lem, id2word=dict_lem, num_topics=num_topics, iterations=1000, random_state=100)\n",
    "    coherencemodel = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "    model_topics.append(num_topics)\n",
    "    model_list.append(lda_reviews_lem)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    print(\"#Topics: \" + str(num_topics) + \" Score: \" + str(coherencemodel.get_coherence()))\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "limit=14; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>- END HERE BECAUSE RESULT ARE NOT GOOD-<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Score - dont think need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence score for lemmatising -tf only\n",
    "coherence_model_lda_lem = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "coherence_lda_lem = coherence_model_lda_lem.get_coherence()\n",
    "print('\\nCoherence Score LDA-lem: ', coherence_lda_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "optimal_model = lda_reviews_lem\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Dominant Topic for each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most dominant topic\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(data)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "#I choose model_list[1] where the number of topics is 4\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=vec_lem, data=doc_lem)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a model to disk, or reload a pre-trained model\n",
    "import pickle\n",
    "\n",
    "# lda_lem_tfidf.save(\"lda\")\n",
    "\n",
    "ldapickle = open('ldapickle', \"wb\")\n",
    "pickle.dump(optimal_model, ldapickle)\n",
    "ldapickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "reader = open(\"ldamallet_model478927.pickle\", \"rb\")\n",
    "tester_model = pickle.load(reader)\n",
    "topics_lem=tester_model.show_topics(10,15)\n",
    "topics_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NEW DATA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list += ['hotel', 'however', 'could', 'get', 'back', 'bit', 'one', 'know', 'i', 'have', 'would', 'take', 'a', 'choose', 'the', 'first', 'second', 'lovely', 'will', 'definitely', 'longer', 'stayed', 'also']\n",
    "\n",
    "def preprocessing(review):\n",
    "    sentences = review.split(\". \")\n",
    "    data = [[word.lower() for word in x.split() if word.lower() not in stop_list] for x in sentences]\n",
    "#     lem = [[lemmatizer.lemmatize(w) for w in doc] for doc in data]\n",
    "    dict_lem=corpora.Dictionary(data)\n",
    "    token_to_id2=dict_lem.token2id\n",
    "    vec_lem= [dict_lem.doc2bow(doc) for doc in data]\n",
    "    \n",
    "    return vec_lem\n",
    "\n",
    "unseen_rev= preprocessing(\"room and bed is spacious. shower old put room water. staff friendly reception room helpful bar.\")\n",
    "\n",
    "for sen in unseen_rev:\n",
    "#     tester_model is the lda model that you load with pickle\n",
    "    result=tester_model[sen]\n",
    "    result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    topic = result[0][0]\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "classifier_saved = open(\"ldamallet_model478927.pickle\", \"rb\") #binary read\n",
    "tester_model = pickle.load(classifier_saved)\n",
    "classifier_saved.close()\n",
    "\n",
    "# TEST NEW DATA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list += ['hotel', 'however', 'could', 'get', 'back', 'bit', 'one', 'know', 'i', 'have', 'would', 'take', 'a', 'choose', 'the', 'first', 'second', 'lovely', 'will', 'definitely', 'longer', 'stayed', 'also']\n",
    "\n",
    "def preprocessing(review):\n",
    "    sentences = review.split(\". \")\n",
    "    data = [[word.lower() for word in x.split() if word.lower() not in stop_list] for x in sentences]\n",
    "#     lem = [[lemmatizer.lemmatize(w) for w in doc] for doc in data]\n",
    "    dict_lem=corpora.Dictionary(data)\n",
    "    token_to_id2=dict_lem.token2id\n",
    "    vec_lem= [dict_lem.doc2bow(doc) for doc in data]\n",
    "    \n",
    "    return vec_lem\n",
    "\n",
    "unseen_rev= preprocessing(\"room and bed is spacious. shower old put room water. staff friendly reception room helpful bar.\")\n",
    "\n",
    "gensim_lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(tester_model)\n",
    "\n",
    "# print(unseen_rev)\n",
    "for sen in unseen_rev:\n",
    "#     tester_model is the lda model that you load with pickle\n",
    "    result=gensim_lda[sen]\n",
    "    result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    topic = result[0][0]\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
