{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import datetime\n",
    "\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('all_opinions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by='date_filed', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "# from pycontractions import Contractions\n",
    "import gensim\n",
    "\n",
    "# Preprocessing\n",
    "from datetime import datetime\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse date\n",
    "dateparse = lambda x: datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "# Read CSV while parsing the dates\n",
    "opinion_df = pd.read_csv('all_opinions.csv', parse_dates=['date_filed'], date_parser=dateparse)\n",
    "\n",
    "# Copy the main df\n",
    "opinion_copy = opinion_df.copy()\n",
    "\n",
    "# Get opinions in the past 50 years\n",
    "above_1970 = opinion_copy[opinion_copy['date_filed'] > \"1970-01-01\"]\n",
    "\n",
    "# Remove Justice Douglas given how his opinions is highly unusual\n",
    "# Refer to https://www.thenation.com/article/archive/tragedy-william-o-douglas/\n",
    "above_1970_no_douglas = above_1970[above_1970['author_name'] != 'Justice Douglas']\n",
    "\n",
    "# Remove those texts with less than 3000 characters as these are recounting past opinions\n",
    "# Refer to https://www.kaggle.com/gqfiddler/scotus-opinions description of the dataset\n",
    "char_above3000_1970 = above_1970_no_douglas[above_1970_no_douglas['text'].str.len() > 3000]\n",
    "\n",
    "# Drop values that are not relevant for our analysis\n",
    "to_analyze = char_above3000_1970.drop(columns=['absolute_url', 'cluster', 'year_filed', \n",
    "                                      'scdb_id', 'date_filed', 'author_name', 'federal_cite_one',\n",
    "                                       'scdb_decision_direction', 'scdb_votes_majority', 'scdb_votes_minority'])\n",
    "# Check the new_df\n",
    "to_analyze = to_analyze.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "\n",
    "def clean(x):\n",
    "    \n",
    "    # Split the case name into array for individual capitalizing\n",
    "    case_name_array = x['case_name'].split()\n",
    "    \n",
    "    # Iterate through the words to capitalize\n",
    "    for i in range(len(case_name_array)):\n",
    "        \n",
    "        # If word not versus, capitalize it to remove later\n",
    "        if case_name_array[i] != 'v.':\n",
    "            case_name_array[i] = case_name_array[i].upper()\n",
    "    \n",
    "    # Join the case name array together\n",
    "    case_name = ' '.join(case_name_array)\n",
    "    \n",
    "    # 1. Standardizing some punctuations\n",
    "    tmp = x['text'].replace('’', \"'\")\n",
    "#     tmp = tmp.replace('“', '\"')\n",
    "#     tmp = tmp.replace('”', '\"')\n",
    "    tmp = tmp.replace('–', \"-\")\n",
    "    tmp = re.sub(r'([.]\\s+){2,10}', '', tmp)\n",
    "    tmp = tmp.replace('[', '')\n",
    "    tmp = tmp.replace(']', '')\n",
    "    \n",
    "    # 2. Remove redundant words\n",
    "    \n",
    "    # A. (i) Remove Cite as: since these are words that keep appearing at the bottom of the transcript for citation\n",
    "    tmp = re.sub(r'Cite as:(.*?)\\((\\d{4})\\)', '', tmp)\n",
    "    \n",
    "    # A. (ii) Remove Opinion of Justice given it is a demarcation of the transcript\n",
    "    tmp = re.sub(r'Opinion\\s(.*?)\\n', '', tmp)\n",
    "    \n",
    "    # B. Remove Case Name\n",
    "    tmp = re.sub(f'''{case_name}''', '', tmp)\n",
    "    \n",
    "    #B. Remove See ... (As these are citations of previous cases to be used)\n",
    "    # Three Cases\n",
    "    # i) See case and citation\n",
    "    see_pattern = re.compile(r\"See(.*?)(\\)\\.|\\)\\;|\\d\\.)\", re.DOTALL)\n",
    "    tmp = re.sub(see_pattern, '', tmp)\n",
    "    \n",
    "    # (ii) Remove quotes i.e. Herring v. New York, 422 U.S. 853, 862 (1975)\n",
    "#     tmp = re.sub(r'', '', tmp)\n",
    "\n",
    "#     print(tmp)\n",
    "    \n",
    "    # 3. Remove unicode characters in text\n",
    "    tmp = re.sub(r'[^\\x00-\\x7F]+', '', tmp)\n",
    "    \n",
    "    # 4. Remove breakline in text\n",
    "    \n",
    "    # A. Embedded in the string (-\\\\n)\n",
    "    tmp = re.sub(r'-\\n\\s{1,}', '', tmp)\n",
    "    tmp = re.sub(r'-\\n', '', tmp) \n",
    "    \n",
    "    # B. Remove long breaks\n",
    "    tmp = re.sub(r'\\n\\s+', ' ', tmp)\n",
    "    \n",
    "    # C. Remove the remaining breaklines\n",
    "    tmp = re.sub(r'\\n', ' ', tmp)\n",
    "    \n",
    "    # 5. Remove Numbers that demarcate sections of opinions\n",
    "    tmp = re.sub(r'\\s\\d{1,}\\s{2,}', ' ', tmp)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# Instantiate the set of stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist += ['u', 'state', 'court', 'id', 'amendment', 'federal', 'respondent', 'appeal', 'case', 'may','could','would', 'c', 'v', 'u', 'one', 'see', 'even', 'issue', 'however', 'supra', 'clause', 'constitutional', 'jury', 'petitioner', 'j', 'requirement', 'ante', 'claim', 'standard', 'process', 'review', 'regulation', 'employee', 'judge', 'criminal', 'n', 'statutory', 'majority', 'individual', 'argument', 'benefit', 'judicial', 'policy', 'result', 'conduct', 'required', 'agency', 'school', 'officer', 'statement', 'violation', 'rather', 'particular', 'ibid', 'circumstance', 'support', 'second', 'protection', 'reasonable', 'party', 'counsel', 'basis', 'clear', 'plan', 'language', 'application', 'sentence', 'well', 'law', 'system', 'member', 'dissenting', 'principle', 'holding', 'need', 'mean', 'procedure', 'although', 'conclusion', 'based', 'private', 'app', 'defendant', 'due', 'practice', 'relief', 'respect', 'since', 'attorney', 'year', 'proceeding', 'prior', 'b', 'legislative', 'provision', 'crime', 'different', 'agreement', 'point', 'inc', 'civil', 'rule', 'provide', 'union', 'today', 'employer', 'purpose', 'way', 'legal', 'decision', 'course', 'child', 'activity', 'finding', 'offense', 'brief', 'statute', 'damage', 'history', 'hearing', 'relevant', 'simply', 'certain', 'conviction', 'ed', 'official', 'remedy', 'require', 'set', 'used', 'interpretation', 'word', 'information', 'police', 'term', 'provides', 'national', 'requires', 'class', 'intended', 'apply', 'concluded', 'death', 'interest', 'example', 'cost', 'place', 'le', 'work', 'hold', 'determination', 'exercise', 'burden', 'ii', 'indeed', 'like', 'say', 'granted', 'limitation', 'emphasis', 'consideration', 'test', 'concurring', 'find', 'take', 'concern', 'person', 'immunity', 'either', 'filed', 'discrimination', 'fact', 'give', 'iii', 'permit', 'three', 'program', 'liability', 'supp', 'meaning', 'petition', 'another', 'applied', 'search', 'report', 'limited', 'challenge', 'added', 'california', 'f', 'motion', 'exception', 'least', 'consider', 'substantial', 'intent', 'defense', 'appropriate', 'fee', 'reason', 'dissent', 'many', 'recognized', 'limit', 'injury', 'nothing', 'l']\n",
    "stopwords_set = set(stoplist)\n",
    "\n",
    "# Instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(x):\n",
    "    \n",
    "    # 1. Lower case\n",
    "    tmp = x.lower()\n",
    "    \n",
    "    # 2. Remove punctuations\n",
    "    tmp = tmp.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Tokenize the sentences\n",
    "    tokens = word_tokenize(tmp)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    no_stopwords = [word for word in tokens if word not in stopwords_set and word.isalpha()]\n",
    "    \n",
    "    # 5. Lemmatize\n",
    "    lemma_text = ' '.join([lemmatizer.lemmatize(word) for word in no_stopwords])\n",
    "    \n",
    "    return lemma_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyze['cleaned_text'] = to_analyze.apply(lambda x : clean(x), axis=1)\n",
    "to_analyze['text'] = to_analyze['cleaned_text'].apply(lambda x: preprocess(x).split(' '))\n",
    "df_lem = to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem = pd.DataFrame({'text' : []})\n",
    "# df_lem['text'] = to_analyze['preprocessed_text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# def no_name(text):\n",
    "#     nlp =  spacy.load('en_core_web_sm')\n",
    "#     doc_no_name_entities = []\n",
    "    \n",
    "#     #Convert into spacy class type to remove name entity\n",
    "#     doc = nlp(text)\n",
    "    \n",
    "#     #Identify name entity in document\n",
    "#     ents = [e.text for e in doc.ents]\n",
    "    \n",
    "#     #Selection of words that do not contain name entity\n",
    "#     for item in doc:\n",
    "#         #Identify name entity in document\n",
    "#         if item.text in ents:\n",
    "#             pass\n",
    "#         else:\n",
    "#             doc_no_name_entities.append(item.text)\n",
    "    \n",
    "#     #Merge word tokens into text\n",
    "#     doc_no_name_entities = \" \".join(doc_no_name_entities)\n",
    "    \n",
    "#     return doc_no_name_entities\n",
    "\n",
    "# df_lem['text'] = df_lem['text'].apply(lambda x: no_name(x))\n",
    "# df_lem['text'] = df_lem['text'].apply(lambda x: [word for word in x.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop all Nan row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop all Nan\n",
    "# df.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample=df.copy(deep=True)\n",
    "# sample = sample[sample['date_filed'] > \"1970-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove special characters (\\n, punctuations, numbers)\n",
    "3. Lemmatisation on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer= WordNetLemmatizer()\n",
    "# start = datetime.datetime.now()\n",
    "\n",
    "# stop_list = stopwords.words('english')\n",
    "\n",
    "# sample['text'] = sample['text'].apply(lambda x: [lemmatizer.lemmatize(re.sub('[^A-Za-z]+', '', word.lower())) for word in x.split()])\n",
    "# print(\"Time taken: \", datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "\n",
    "# doc_lem = sample['text'].values.tolist()\n",
    "\n",
    "# a = []\n",
    "\n",
    "# for i in doc_lem:\n",
    "#     a += i\n",
    "\n",
    "# # Pass the split_it list to instance of Counter class. \n",
    "# Counter = Counter(a) \n",
    "  \n",
    "# # most_common() produces k frequently encountered \n",
    "# # input values and their respective counts. \n",
    "# most_occur = Counter.most_common(200) \n",
    "\n",
    "# def Convert(tup, di): \n",
    "#     for a, b in tup: \n",
    "#         di.setdefault(a, []).append(b) \n",
    "#     return di \n",
    "\n",
    "# dictionary = {}\n",
    "# words = Convert(most_occur, dictionary)\n",
    "# print(words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Removing stopwords and splitting into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # law corpus\n",
    "# # stop_list += ['section', 'relevant', 'period', 'submission', 'grant', 'appropriate', 'behalf', 'convention', 'assessment', 'process', 'commissioner', 'previous', 'enable', 'transfer', 'justify', 'specific', 'comment', 'contact', 'final', 'error', 'confirm', 'interpretation', 'discrimination', 'create', 'subsequent', 'sexual', 'compensation', 'capable', 'objective', 'attach', 'category', 'obvious', 'feature', 'considerable', 'subsequently', 'instruction', 'confer', 'summarise', 'fee', 'link', 'investigate', 'overall', 'resolution', 'enforce', 'initial', 'recovery', 'sole', 'inference', 'maintenance', 'regime', 'priority', 'residential', 'obviously', 'respond', 'couple', 'maximum', 'welfare', 'potentially', 'temporary', 'attribute', 'participate', 'correspondence', 'whereas', 'promote', 'solely', 'deduction', 'technical', 'finally', 'violation', 'exceed', 'primarily', 'brief', 'eventually', 'recoverable', 'compatible', 'annual', 'disproportionate', 'communicate', 'military', 'hence', 'core', 'consumer', 'facilitate', 'assure', 'reverse', 'adjustment', 'restrain', 'environment', 'formulate', 'briefly', 'institution', 'intervention', 'foundation', 'identical', 'evidential', 'legality', 'enhance', 'trigger', 'specification', 'regulatory', 'presumably', 'practitioner', 'appreciation', 'presume', 'index', 'constructive', 'analogy', 'alternatively', 'appendix', 'unaware', 'clarity', 'accurately', 'trace', 'consequently', 'adapt', 'via', 'conversion', 'reluctant', 'precedent', 'integrity', 'appropriately', 'revision', 'equate', 'occurrence', 'attain', 'inhibit', 'co-operation', 'contradict', 'pursuit', 'isolate', 'explicit', 'co-operate', 'consistency', 'classic', 'displace', 'constrain', 'dominant', 'constant', 'administrator', 'compatibility', 'flexible', 'conceive', 'evidently', 'theme', 'incentive', 'uniform', 'commentary', 'mode', 'succession', 'temporarily', 'detective', 'computation', 'regulator', 'volunteer', 'periodic', 'mature', 'controversy', 'constraint', 'mutually', 'sphere', 'evolve', 'rationality', 'journal', 'unaccompanied', 'exploit', 'tradition', 'concentration', 'retainer', 'transmission', 'survivor', 'denote', 'style', 'institutional', 'sustainable', 'enormous', 'trend', 'norm', 'cooperate', 'demonstrably', 'constituent', 'professionally', 'variable', 'abandonment', 'interpretative', 'radically', 'appreciable', 'negate', 'chart', 'reversal', 'lecture', 'estimation', 'adaptation', 'traditionally', 'random', 'medically', 'erroneously', 'linkage', 'philosophy', 'proportionately', 'exporter', 'predictable', 'legislator', 'proportional', 'convening', 'vision', 'emphasize', 'periodically', 'founder', 'selective', 'irreversible', 'subsidy', 'crucially', 'relaxation', 'instability', 'ethics', 'revelation', 'co-ordination', 'publisher', 'phenomenon', 'incapacitate', 'nuclear', 'obtainable', 'compilation', 'philosophical', 'immature', 'liberation', 'demonstrator', 'statistically', 'dynamics', 'transited', 'constitutive', 'subordination', 'thesis', 'survival', 'neutralise', 'variability', 'lecturer', 'economist', 'traceable', 'incoherent', 'resourceful', 'constitutionally', 'automate', 'unethical', 'randomness', 'constituency', 'institutionally', 'creative', 'readjust', 'expansive', 'coherently', 'undeniable', 'computational', 'unconventional', 'academy', 'redraft', 'recreate', 'constancy', 'attainable', 'conformable', 'analytically', 'unparalleled', 'restrictively', 'funder', 'expertly', 'minimize', 'chemically', 'insightful', 'unattached', 'ideological', 'unpublished', 'inaccessible', 'transferable', 'contextualize', 'schematically', 'institutionalise', 'evidence', 'authority', 'submit', 'conclusion', 'approach', 'conduct', 'identify', 'policy', 'individual', 'indicate', 'sufficient', 'significant', 'investigation', 'release', 'commission', 'community', 'assume', 'specify', 'purchase', 'scope', 'response', 'site', 'achieve', 'aspect', 'income', 'register', 'assist', 'role', 'analysis', 'criterion', 'fundamental', 'majority', 'procedural', 'impact', 'apparent', 'outcome', 'incident', 'cease', 'inconsistent', 'interpret', 'commence', 'inspector', 'specifically', 'data', 'normally', 'task', 'significance', 'notwithstanding', 'adequate', 'method', 'indication', 'sustain', 'consist', 'framework', 'presumption', 'dispose', 'restraint', 'reside', 'implication', 'series', 'contribute', 'variation', 'prohibit', 'irrelevant', 'proportionate', 'inevitably', 'occupation', 'exclusion', 'inappropriate', 'distribution', 'validity', 'disposal', 'beneficial', 'attributable', 'adult', 'extract', 'consult', 'hypothetical', 'commencement', 'imposition', 'margin', 'environmental', 'legislature', 'perceive', 'emphasis', 'approximately', 'select', 'assignment', 'assign', 'mechanism', 'transitional', 'automatically', 'technology', 'occupier', 'ministry', 'selection', 'adequately', 'indefinite', 'inadequate', 'retention', 'exclusively', 'undergo', 'survey', 'aggregate', 'construct', 'label', 'incapable', 'distribute', 'reaction', 'assistant', 'survive', 'commitment', 'locate', 'highlight', 'device', 'image', 'supplement', 'conference', 'attachment', 'levy', 'manual', 'evaluation', 'legislate', 'logical', 'evident', 'occupational', 'acknowledgement', 'flexibility', 'evaluate', 'so-called', 'restoration', 'complexity', 'portion', 'overlap', 'technique', 'logically', 'motivate', 'furthermore', 'eventual', 'predict', 'economically', 'implicate', 'successor', 'region', 'abstract', 'scenario', 'invariably', 'tape', 'authoritative', 'ratio', 'conformity', 'interaction', 'widespread', 'indicative', 'precision', 'tension', 'justifiable', 'extraction', 'principled', 'currency', 'collapse', 'technically', 'utilise', 'investor', 'focusing', 'invalidity', 'deduce', 'illogical', 'comprehensively', 'persistent', 'maturity', 'finalise', 'odds', 'unambiguous', 'ambiguous', 'compile', 'append', 'incidence', 'insufficiently', 'instructive', 'goal', 'transform', 'coincidence', 'decade', 'definitely', 'approximate', 'diversity', 'domain', 'unaffected', 'liberal', 'precedence', 'inaccuracy', 'prioritisation', 'passive', 'complement', 'reassessment', 'fluctuate', 'reformulate', 'disproportionately', 'attainment', 'deviation', 'eventuality', 'intrinsically', 'marginally', 'visual', 'reluctantly', 'persistently', 'abnormal', 'domesticate', 'theoretically', 'textual', 'abnormally', 'uniformity', 'convincingly', 'financier', 'validation', 'reformulation', 'sexuality', 'elimination', 'structurally', 'designer', 'projection', 'identically', 'restructuring', 'uniquely', 'visibility', 'incidentally', 'itemise', 'insecure', 'analytical', 'coordination', 'technological', 'dynamic', 'debatable', 'aggregation', 'preliminaries', 'securely', 'editorial', 'unaltered', 'innovative', 'unconvinced', 'misinterpretation', 'enormity', 'visualise', 'responsive', 'sequentially', 'misinterpreting', 'cyclical', 'reassess', 'intensely', 'methodical', 'predominate', 'energetically', 'psychologically', 'analyze', 'reoccur', 'rigidity', 'dominance', 'domination', 'hypothesise', 'evolutionary', 'unstructured', 'unanticipated', 'migrate', 'finalize', 'symbolic', 'coincident', 'regionally', 'uniqueness', 'illogically', 'utilization', 'indiscretion', 'transference', 'individualist', 'methodological', 'issue', 'circumstance', 'regulation', 'involve', 'legal', 'obtain', 'document', 'available', 'context', 'area', 'immigration', 'consequence', 'constitute', 'financial', 'ensure', 'medical', 'assess', 'construction', 'occur', 'amend', 'remove', 'alternative', 'function', 'amendment', 'aware', 'mental', 'consistent', 'demonstrate', 'pursue', 'cite', 'file', 'resolve', 'distinction', 'acknowledge', 'previously', 'restriction', 'option', 'status', 'estate', 'professional', 'conflict', 'sufficiently', 'imply', 'creditor', 'concept', 'positive', 'underlie', 'confine', 'legislative', 'apparently', 'decline', 'identity', 'partner', 'route', 'valid', 'somewhat', 'incompatible', 'finance', 'sex', 'initially', 'pose', 'substitute', 'alter', 'implement', 'investment', 'legally', 'ignore', 'exclusive', 'contractor', 'consultant', 'stress', 'corporate', 'guideline', 'expose', 'documentation', 'transport', 'crucial', 'prohibition', 'protocol', 'formulation', 'analyse', 'labour', 'distinct', 'ultimate', 'beneficiary', 'ethnic', 'establishment', 'discriminate', 'negative', 'reinforce', 'notion', 'insert', 'violate', 'author', 'entity', 'participation', 'compensate', 'inevitable', 'minority', 'definitive', 'parallel', 'duration', 'incline', 'psychological', 'display', 'diminish', 'inconsistency', 'intelligence', 'alteration', 'irrational', 'expertise', 'consequent', 'validly', 'reliability', 'subordinate', 'theory', 'bias', 'export', 'conclusive', 'rejection', 'exhibit', 'perception', 'supplementary', 'assembly', 'inspect', 'cycle', 'adjacent', 'psychologist', 'explicitly', 'categorise', 'gender', 'exposure', 'similarity', 'encounter', 'unjustified', 'motivation', 'illegality', 'federal', 'positively', 'substitution', 'channel', 'stability', 'depression', 'insight', 'definite', 'accessible', 'functional', 'persist', 'bulk', 'distort', 'occupant', 'compute', 'predominant', 'ministerial', 'conceivable', 'unfounded', 'rigid', 'restructure', 'theoretical', 'illegally', 'terminal', 'inconceivable', 'psychology', 'maximise', 'relocate', 'demonstration', 'culture', 'insertion', 'migrant', 'orientation', 'transmit', 'validate', 'marginal', 'conclusively', 'constantly', 'anticipation', 'economy', 'indicator', 'reliant', 'detection', 'relax', 'inflexible', 'enhancement', 'format', 'predominantly', 'intelligent', 'prediction', 'primacy', 'consultancy', 'signify', 'justifiably', 'exclusionary', 'correspondingly', 'ethnicity', 'distinctly', 'contradiction', 'occupancy', 'misinterpret', 'unify', 'distortion', 'expansion', 'integration', 'underestimate', 'equivalence', 'variant', 'specificity', 'unambiguously', 'annually', 'conceptually', 'predictability', 'advocacy', 'achievable', 'contributor', 'complementary', 'stressful','abstraction', 'differentiation', 'subsidise', 'persistence', 'unprincipled', 'unbiased', 'reactivate', 'prohibitive', 'unconstrained', 'undefined', 'co-ordinate', 'subordinates', 'mediate', 'classical', 'challenger', 'distributive', 'unprecedented', 'rigidly', 'emergence', 'facilitator', 're-evaluation', 'commodity', 'liberally', 'coordinate', 'negatively', 'unregulated', 'max', 'reactive', 'derivation', 'transitory', 'intelligently', 'professionalism', 'devotion', 'affective', 'normality', 'paralleled', 'consultative', 'inflexibility', 'bulky', 'creator', 'stylise', 'visually', 'mentality', 'exhibition', 'unalterable', 'functionally', 'accompaniment', 'diversification', 'reactor', 'ideology', 'innovator', 'creativity', 'reversible', 'unmodified', 'inconstancy', 'adaptability', 'overestimate', 'unresponsive', 'normalisation', 'proportionally', 'paragraph', 'seek', 'principle', 'respondent', 'rely', 'requirement', 'clause', 'schedule', 'proceed', 'challenge', 'commit', 'legislation', 'similar', 'secure', 'sum', 'injury', 'contrary', 'prior', 'domestic', 'define', 'definition', 'acquire', 'summary', 'primary', 'panel', 'recover', 'expert', 'element', 'physical', 'assumption', 'administrative', 'design', 'despite', 'instance', 'consent', 'reliance', 'quote', 'economic', 'resident', 'residence', 'occupy', 'capacity', 'minimum', 'registration', 'vary', 'publish', 'derive', 'albeit', 'nevertheless', 'corporation', 'terminate', 'structure', 'involvement', 'communication', 'network', 'consultation', 'relevance', 'license', 'precisely', 'estimate', 'ultimately', 'appreciate', 'revise', 'debate', 'accompany', 'contrast', 'inherent', 'purchaser', 'subsidiary', 'project', 'internal', 'invoke', 'reveal', 'team', 'emerge', 'insufficient', 'allocate', 'infer', 'computer', 'monitor', 'proportion', 'complex', 'intervene', 'consistently', 'denial', 'target', 'whereby', 'medium', 'modification', 'anticipate', 'precede', 'accurate', 'generate', 'external', 'constitutional', 'institute', 'motive', 'implementation', 'availability', 'constitution', 'clarify', 'mutual', 'citation', 'objectively', 'ongoing', 'implicit', 'initiate', 'straightforward', 'conventional', 'restore', 'grade', 'prime', 'virtually', 'traditional', 'confirmation', 'formula', 'expand', 'overseas', 'incompatibility', 'intermediate', 'undertaking', 'physically', 'fundamentally', 'compensatory', 'likewise', 'eliminate', 'colleague', 'volume', 'participant', 'voluntarily', 'sector', 'interval', 'minimise', 'successive', 'clarification', 'investigator', 'statistics', 'capability', 'sexually', 'offset', 'conform', 'financially', 'invalidate', 'cultural', 'illustration', 'hypothesis', 'inadequacy', 'ambiguity', 'minimal', 'transit', 'isolation', 'utility', 'react', 'investigative', 'distributor', 'diminution', 'coincide', 'global', 'editor', 'conceivably', 'relocation', 'energy', 'cooperation', 'promoter', 'intensive', 'coherent', 'intense', 'integrate', 'assemble', 'differentiate', 'distinctive', 'generation', 'mediation', 'indefinitely', 'rationally', 'co-operative', 'finality', 'unlicensed', 'accumulate', 'intensity', 'refine', 'internally', 'concurrently', 'immigrant', 'variance', 'methodology', 'reconstruction', 'converse', 'inappropriately', 'intrinsic', 'sufficiency', 'illustrative', 'uncontroversial', 'evolution', 'evaluative', 'diverse', 'conceptual', 'unpredictable', 'finite', 'innovation', 'imprecise', 'qualitative', 'tense', 'reconstruct', 'dominate', 'demonstrable', 'rationalisation', 'underlay', 'dramatically', 'unpredictability', 'reliably', 'conception', 'cooperative', 'transportation', 'contextual', 'objectivity', 'brevity', 'inhibition', 'accumulation', 'accessibility', 'coherence', 'appreciably', 'rationalise', 'unsustainable', 'uniformly', 'collapsible', 'approximation', 'virtual', 'intensify', 'infinitely', 'inconclusive', 'environmentally', 'unaided', 'summarize', 'selectively', 'reinforcement', 'definable', 'minimally', 'disposable', 'researcher', 'successively', 'analyst', 'redefine', 'detectable', 'predictably', 'hypothetically', 'utilize', 'globally', 'empirical', 'removable', 'revolution', 'displacement', 'unintelligent', 'ethic', 'imagery', 'innovate', 'adulthood', 'stabilise', 'immaturity', 'unscheduled', 'predominance', 'conceptualise', 'intensification', 'briefing', 'manually', 'passivity', 'discretely', 'simulation', 'coordinator', 'itemisation', 'co-ordinator', 'reactivation', 'unrestrained', 'participatory', 'unidentifiable', 'require', 'proceeding', 'benefit', 'contract', 'conclude', 'establish', 'security', 'procedure', 'impose', 'factor', 'scheme', 'reject', 'access', 'affect', 'draft', 'civil', 'discretion', 'exclude', 'fund', 'undertake', 'principal', 'maintain', 'potential', 'accommodation', 'deny', 'advocate', 'licence', 'prospect', 'assistance', 'credit', 'vehicle', 'guarantee', 'enforcement', 'instruct', 'removal', 'focus', 'equipment', 'retain', 'range', 'item', 'preliminary', 'normal', 'restrict', 'contribution', 'code', 'incorporate', 'emphasise', 'source', 'suspend', 'revenue', 'resource', 'facility', 'justification', 'chapter', 'administration', 'assurance', 'equivalent', 'precise', 'aid', 'version', 'publication', 'thereby', 'comprise', 'acquisition', 'significantly', 'injure', 'job', 'illegal', 'major', 'component', 'termination', 'identification', 'partnership', 'minor', 'suspension', 'allocation', 'discretionary', 'text', 'prospective', 'location', 'nonetheless', 'regulate', 'modify', 'abandon', 'similarly', 'bond', 'correspond', 'illustrate', 'attitude', 'accommodate', 'percentage', 'comprehensive', 'research', 'concentrate', 'convert', 'reliable', 'input', 'voluntary', 'inspection', 'plus', 'inherently', 'regional', 'rational', 'convince', 'concurrent', 'automatic', 'principally', 'detect', 'induce', 'stable', 'academic', 'edition', 'restrictive', 'topic', 'considerably', 'analogous', 'quotation', 'arbitrary', 'devote', 'shift', 'creation', 'incorporation', 'vol', 'adjust', 'sequence', 'attribution', 'erroneous', 'periodical', 'identifiable', 'logic', 'chemical', 'strategy', 'phase', 'invest', 'discrete', 'odd', 'adequacy', 'implicitly', 'promotion', 'compound', 'neutral', 'individually', 'integral', 'mentally', 'controversial', 'structural', 'accuracy', 'derivative', 'perspective', 'consensus', 'inaccurate', 'awareness', 'dimension', 'convene', 'forthcoming', 'consumption', 'ignorance', 'visible', 'output', 'appropriateness', 'reluctance', 'consume', 'contemporary', 'unique', 'strategic', 'achievement', 'radical', 'initiative', 'unspecified', 'equip', 'insignificant', 'exploitation', 'prioritise', 'depress', 'commentator', 'categorisation', 'paradigm', 'unresolved', 'unreliable', 'transition', 'interact', 'manipulate', 'infrastructure', 'parameter', 'assessable', 'unavailable', 'edit', 'ignorant', 'layer', 'dissimilar', 'contradictory', 'unrestricted', 'globe', 'hierarchy', 'refinement', 'erosion', 'neutrality', 'conventionally', 'dramatic', 'inclination', 'induction', 'statistical', 'equation', 'domestically', 'unstable', 'manipulation', 'deviate', 'schematic', 'inadequately', 'ethical', 'alternate', 'federation', 'fluctuation', 'erode', 'conversely', 'emphatically', 'optional', 'invariable', 'hierarchical', 'administratively', 'enormously', 'arbitrarily', 'arbitrariness', 'emphatic', 'economical', 're-evaluate', 'inevitability', 'detector', 'migration', 'irrelevance', 'manipulative', 'unconstitutional', 'infinite', 'instructor', 'utilisation', 'symbol', 'economics', 'orientate', 'externally', 'convertible', 'transformation', 'visibly', 'reinvest', 'initiation', 'coincidental', 'redistribution', 'academia', 'liberate', 'energetic', 'authorship', 'sustenance', 'facilitation', 'sustainability', 'orient', 'percent', 'randomly', 'assuredly', 'symbolise', 'sequential', 'academically', 'redistribute', 'individuality', 'drama', 'emergent', 'maximize', 'statistic', 'indistinct', 'unassessed', 'dimensional', 'prioritises', 'conformation', 'readjustment', 'contextualise', 'revolutionary', 'controversially']\n",
    "\n",
    "# # eyeball + top 100 common\n",
    "# stop_list += ['court', 'law', 'claim', 'state', 'made', 'case', 'act', 'city', 'right', 'rights', 'defendant', 'may', \n",
    "#                 'the', 'of', '', 'to', 'and', 'a', 'in', 'that', 'it', 'is', 'by', 'wa', 'be', 'for', 'or', 'not', 'court', 'state', 'which', 'this', 'on', 'v', 'from', 'with', 'an', 'case', 'upon', 'such', 'act', 'u', 'any', 'are', 'no', 'at', 'were', 'but', 'have', 'had', 'under', 'been', 'law', 'company', 'united', 'we', 'may', 'his', 'right', 'if', 'other', 'made', 'there', 'all', 'he', 'ha', 'said', 'would', 'one', 'question', 'so', 'statute', 'their', 'they', 'co', 'property', 'power', 'land', 'shall', 'judgment', 'tax', 'within', 'fact', 'defendant', 'order', 'only', 'against', 'time', 'when', 'should', 'contract', 'plaintiff', 'these', 'same', 'them', 'claim', 'appeal', 'purpose', 'district', 'without', 'congress', 'action', 'commission', 'new', 'provision', 'jurisdiction', 'held', 'whether', 'than', 'must', 'section', 'before', 'after', 'part', 'decision', 'suit', 'could', 'corporation', 'railroad', 'will', 'rate', 'those', 'did', 'public', 'between', 'general', 'government', 'subject', 'commerce', 'evidence', 'business', 'year', 'because', 'party', 'opinion', 'circuit', 'federal', 'who', 'where', 'into', 'person', 'effect', 'also', 'authority', 'doe', 'error', 'make', 'proceeding', 'interest', 'rule', 'amount', 'use', 'city', 'him', 'constitution', 'here', 'bank', 'do', 'first', 'more', 'being', 'out', 'interstate', 'petitioner', 'what', 'decree', 'two', 'bill', 'ground', 'value', 'can', 'sale', 'might', 'then', 'some', 'therefore', 'term', 'line', 'cannot', 'matter', 'c', 'over', 'cause', 'supreme', 'each', 'stat', 'justice', 'given', 'title', 'duty', 'service', 'present', 'further', 'provided', 'condition', 'paid', 'carrier', 'reason', 'taken', 'found', 'trial', 'brought', 'necessary', 'payment', 'board', 'mr', 'record', 'stock', 'construction', 'view', 'patent', 'thus']\n",
    "\n",
    "# stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample['text'] = sample['text'].apply(lambda x: list(set(x) - stop_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = sample['text'].values.tolist()\n",
    "# bi = gensim.models.phrases.Phrases(doc, min_count=3, threshold=10)\n",
    "# tri = gensim.models.phrases.Phrases(bi[doc], min_count=3, threshold=10)\n",
    "# sample['text'] = tri[bi[doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lem=sample.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1) Create dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.1) Create list from dataframe <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df lemmatise only\n",
    "doc_lem= df_lem['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.2) Create dictionary <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lem=corpora.Dictionary(doc_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id2=dict_lem.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.3) TF <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lem= [dict_lem.doc2bow(doc) for doc in doc_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA model with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) LDA with term freq (tf)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:00:21.114503\n",
      "\n",
      "Coherence Score LDA-lem:  0.3249501893265598\n",
      "[(0,\n",
      "  '0.012*\"u\" + 0.009*\"act\" + 0.007*\"state\" + 0.007*\"congress\" + 0.006*\"united\" '\n",
      "  '+ 0.004*\"board\" + 0.004*\"government\" + 0.004*\"whether\" + 0.004*\"first\" + '\n",
      "  '0.003*\"must\"'),\n",
      " (1,\n",
      "  '0.012*\"u\" + 0.010*\"state\" + 0.008*\"act\" + 0.007*\"united\" + 0.006*\"land\" + '\n",
      "  '0.005*\"right\" + 0.005*\"water\" + 0.005*\"congress\" + 0.005*\"new\" + '\n",
      "  '0.004*\"power\"'),\n",
      " (2,\n",
      "  '0.012*\"u\" + 0.011*\"tax\" + 0.008*\"act\" + 0.008*\"congress\" + 0.008*\"state\" + '\n",
      "  '0.006*\"property\" + 0.004*\"government\" + 0.004*\"united\" + 0.004*\"court\" + '\n",
      "  '0.004*\"whether\"'),\n",
      " (3,\n",
      "  '0.012*\"u\" + 0.007*\"right\" + 0.007*\"district\" + 0.005*\"public\" + '\n",
      "  '0.005*\"state\" + 0.005*\"first\" + 0.004*\"court\" + 0.004*\"government\" + '\n",
      "  '0.004*\"must\" + 0.004*\"speech\"'),\n",
      " (4,\n",
      "  '0.014*\"u\" + 0.012*\"court\" + 0.010*\"district\" + 0.008*\"action\" + '\n",
      "  '0.008*\"state\" + 0.006*\"jurisdiction\" + 0.006*\"act\" + 0.006*\"congress\" + '\n",
      "  '0.006*\"right\" + 0.006*\"judgment\"'),\n",
      " (5,\n",
      "  '0.014*\"u\" + 0.010*\"trial\" + 0.008*\"evidence\" + 0.006*\"state\" + '\n",
      "  '0.006*\"court\" + 0.005*\"right\" + 0.005*\"whether\" + 0.004*\"united\" + '\n",
      "  '0.004*\"question\" + 0.004*\"case\"')]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lda_reviews_lem= gensim.models.ldamodel.LdaModel(corpus=vec_lem, id2word=dict_lem, num_topics=6, random_state=100, alpha=0.001)\n",
    "print(\"Time taken: \", datetime.now()-start)\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence score for lemmatising -tf only\n",
    "coherence_model_lda_lem = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "coherence_lda_lem = coherence_model_lda_lem.get_coherence()\n",
    "print('\\nCoherence Score LDA-lem: ', coherence_lda_lem)\n",
    "\n",
    "from pprint import pprint\n",
    "optimal_model = lda_reviews_lem\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_lem=lda_reviews_lem.show_topics(12,15)\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     print(topics_lem[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign topic back to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem['topic'] = [optimal_model.get_document_topics(dict_lem.doc2bow(doc)) for doc in doc_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_lem.head(5)\n",
    "# possible topic of 4 and 5, adding probability to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a model to disk, or reload a pre-trained model\n",
    "import pickle\n",
    "\n",
    "ldapickle = open('bryclean_lda', \"wb\")\n",
    "pickle.dump(optimal_model, ldapickle)\n",
    "ldapickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----END----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Optimal Number of the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Model with tf vectors - DONT RUN THIS<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-31 17:39:58.169159\n",
      "#Topics: 2 Score: 0.2932776510606139\n",
      "#Topics: 4 Score: 0.30282549803003306\n",
      "#Topics: 6 Score: 0.3267468800148175\n",
      "#Topics: 8 Score: 0.32674872904896357\n",
      "#Topics: 10 Score: 0.33264678120869473\n",
      "#Topics: 12 Score: 0.3373141814939637\n",
      "2020-10-31 18:05:10.004498\n"
     ]
    }
   ],
   "source": [
    "# Can take a long time to run. In this case we are going to  k_max=10.\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "model_list = []\n",
    "coherence_values = []\n",
    "model_topics = []\n",
    "\n",
    "for num_topics in range(2, 14, 2):\n",
    "    #sg_lda_x = gensim.models.ldamodel.LdaModel(corpus=sg_vecs, id2word=sg_dictionary, num_topics=num_topics)\n",
    "    lda_reviews_lem= gensim.models.ldamodel.LdaModel(corpus=vec_lem, id2word=dict_lem, num_topics=num_topics, iterations=1000, random_state=100)\n",
    "    coherencemodel = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "    model_topics.append(num_topics)\n",
    "    model_list.append(lda_reviews_lem)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    print(\"#Topics: \" + str(num_topics) + \" Score: \" + str(coherencemodel.get_coherence()))\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "limit=14; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 2  has Coherence Value of 0.2933\n",
      "Num Topics = 4  has Coherence Value of 0.3028\n",
      "Num Topics = 6  has Coherence Value of 0.3267\n",
      "Num Topics = 8  has Coherence Value of 0.3267\n",
      "Num Topics = 10  has Coherence Value of 0.3326\n",
      "Num Topics = 12  has Coherence Value of 0.3373\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>- END HERE BECAUSE RESULT ARE NOT GOOD-<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Score - dont think need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence score for lemmatising -tf only\n",
    "coherence_model_lda_lem = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "coherence_lda_lem = coherence_model_lda_lem.get_coherence()\n",
    "print('\\nCoherence Score LDA-lem: ', coherence_lda_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Dominant Topic for each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most dominant topic\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(data)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "#I choose model_list[1] where the number of topics is 4\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=vec_lem, data=doc_lem)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a model to disk, or reload a pre-trained model\n",
    "import pickle\n",
    "\n",
    "# lda_lem_tfidf.save(\"lda\")\n",
    "\n",
    "ldapickle = open('ldapickle', \"wb\")\n",
    "pickle.dump(optimal_model, ldapickle)\n",
    "ldapickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "reader = open(\"ldamallet_model478927.pickle\", \"rb\")\n",
    "tester_model = pickle.load(reader)\n",
    "topics_lem=tester_model.show_topics(10,15)\n",
    "topics_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NEW DATA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list += ['hotel', 'however', 'could', 'get', 'back', 'bit', 'one', 'know', 'i', 'have', 'would', 'take', 'a', 'choose', 'the', 'first', 'second', 'lovely', 'will', 'definitely', 'longer', 'stayed', 'also']\n",
    "\n",
    "def preprocessing(review):\n",
    "    sentences = review.split(\". \")\n",
    "    data = [[word.lower() for word in x.split() if word.lower() not in stop_list] for x in sentences]\n",
    "#     lem = [[lemmatizer.lemmatize(w) for w in doc] for doc in data]\n",
    "    dict_lem=corpora.Dictionary(data)\n",
    "    token_to_id2=dict_lem.token2id\n",
    "    vec_lem= [dict_lem.doc2bow(doc) for doc in data]\n",
    "    \n",
    "    return vec_lem\n",
    "\n",
    "unseen_rev= preprocessing(\"room and bed is spacious. shower old put room water. staff friendly reception room helpful bar.\")\n",
    "\n",
    "for sen in unseen_rev:\n",
    "#     tester_model is the lda model that you load with pickle\n",
    "    result=tester_model[sen]\n",
    "    result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    topic = result[0][0]\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "classifier_saved = open(\"ldamallet_model478927.pickle\", \"rb\") #binary read\n",
    "tester_model = pickle.load(classifier_saved)\n",
    "classifier_saved.close()\n",
    "\n",
    "# TEST NEW DATA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list += ['hotel', 'however', 'could', 'get', 'back', 'bit', 'one', 'know', 'i', 'have', 'would', 'take', 'a', 'choose', 'the', 'first', 'second', 'lovely', 'will', 'definitely', 'longer', 'stayed', 'also']\n",
    "\n",
    "def preprocessing(review):\n",
    "    sentences = review.split(\". \")\n",
    "    data = [[word.lower() for word in x.split() if word.lower() not in stop_list] for x in sentences]\n",
    "#     lem = [[lemmatizer.lemmatize(w) for w in doc] for doc in data]\n",
    "    dict_lem=corpora.Dictionary(data)\n",
    "    token_to_id2=dict_lem.token2id\n",
    "    vec_lem= [dict_lem.doc2bow(doc) for doc in data]\n",
    "    \n",
    "    return vec_lem\n",
    "\n",
    "unseen_rev= preprocessing(\"room and bed is spacious. shower old put room water. staff friendly reception room helpful bar.\")\n",
    "\n",
    "gensim_lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(tester_model)\n",
    "\n",
    "# print(unseen_rev)\n",
    "for sen in unseen_rev:\n",
    "#     tester_model is the lda model that you load with pickle\n",
    "    result=gensim_lda[sen]\n",
    "    result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    topic = result[0][0]\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
