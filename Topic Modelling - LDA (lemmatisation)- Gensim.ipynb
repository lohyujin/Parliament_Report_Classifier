{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import datetime\n",
    "\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_opinions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop all Nan row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all Nan\n",
    "df.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=df.copy(deep=True)\n",
    "samples = samples.sort_values(by='date_filed', ascending=False)\n",
    "sample=samples[:9000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove special characters (\\n, punctuations, numbers)\n",
    "3. Lemmatisation on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:02:03.659810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yujin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "lemmatizer= WordNetLemmatizer()\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "\n",
    "sample['text'] = sample['text'].apply(lambda x: [lemmatizer.lemmatize(re.sub('[^A-Za-z]+', '', word.lower())) for word in x.split()])\n",
    "print(\"Time taken: \", datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['the', 'of', '', 'to', 'and', 'a', 'in', 'that', 'it', 'is', 'by', 'wa', 'be', 'for', 'or', 'not', 'court', 'state', 'which', 'this', 'on', 'v', 'from', 'with', 'an', 'case', 'upon', 'such', 'act', 'u', 'any', 'are', 'no', 'at', 'were', 'but', 'have', 'had', 'under', 'been', 'law', 'company', 'united', 'we', 'may', 'his', 'right', 'if', 'other', 'made', 'there', 'all', 'he', 'ha', 'said', 'would', 'one', 'question', 'so', 'statute', 'their', 'they', 'co', 'property', 'power', 'land', 'shall', 'judgment', 'tax', 'within', 'fact', 'defendant', 'order', 'only', 'against', 'time', 'when', 'should', 'contract', 'plaintiff', 'these', 'same', 'them', 'claim', 'appeal', 'purpose', 'district', 'without', 'congress', 'action', 'commission', 'new', 'provision', 'jurisdiction', 'held', 'whether', 'than', 'must', 'section', 'before', 'after', 'part', 'decision', 'suit', 'could', 'corporation', 'railroad', 'will', 'rate', 'those', 'did', 'public', 'between', 'general', 'government', 'subject', 'commerce', 'evidence', 'business', 'year', 'because', 'party', 'opinion', 'circuit', 'federal', 'who', 'where', 'into', 'person', 'effect', 'also', 'authority', 'doe', 'error', 'make', 'proceeding', 'interest', 'rule', 'amount', 'use', 'city', 'him', 'constitution', 'here', 'bank', 'do', 'first', 'more', 'being', 'out', 'interstate', 'petitioner', 'what', 'decree', 'two', 'bill', 'ground', 'value', 'can', 'sale', 'might', 'then', 'some', 'therefore', 'term', 'line', 'cannot', 'matter', 'c', 'over', 'cause', 'supreme', 'each', 'stat', 'justice', 'given', 'title', 'duty', 'service', 'present', 'further', 'provided', 'condition', 'paid', 'carrier', 'reason', 'taken', 'found', 'trial', 'brought', 'necessary', 'payment', 'board', 'mr', 'record', 'stock', 'construction', 'view', 'patent', 'thus'])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "doc_lem = sample['text'].values.tolist()\n",
    "\n",
    "a = []\n",
    "\n",
    "for i in doc_lem:\n",
    "    a += i\n",
    "\n",
    "# Pass the split_it list to instance of Counter class. \n",
    "Counter = Counter(a) \n",
    "  \n",
    "# most_common() produces k frequently encountered \n",
    "# input values and their respective counts. \n",
    "most_occur = Counter.most_common(200) \n",
    "\n",
    "def Convert(tup, di): \n",
    "    for a, b in tup: \n",
    "        di.setdefault(a, []).append(b) \n",
    "    return di \n",
    "\n",
    "dictionary = {}\n",
    "words = Convert(most_occur, dictionary)\n",
    "print(words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Removing stopwords and splitting into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# law corpus\n",
    "# stop_list += ['section', 'relevant', 'period', 'submission', 'grant', 'appropriate', 'behalf', 'convention', 'assessment', 'process', 'commissioner', 'previous', 'enable', 'transfer', 'justify', 'specific', 'comment', 'contact', 'final', 'error', 'confirm', 'interpretation', 'discrimination', 'create', 'subsequent', 'sexual', 'compensation', 'capable', 'objective', 'attach', 'category', 'obvious', 'feature', 'considerable', 'subsequently', 'instruction', 'confer', 'summarise', 'fee', 'link', 'investigate', 'overall', 'resolution', 'enforce', 'initial', 'recovery', 'sole', 'inference', 'maintenance', 'regime', 'priority', 'residential', 'obviously', 'respond', 'couple', 'maximum', 'welfare', 'potentially', 'temporary', 'attribute', 'participate', 'correspondence', 'whereas', 'promote', 'solely', 'deduction', 'technical', 'finally', 'violation', 'exceed', 'primarily', 'brief', 'eventually', 'recoverable', 'compatible', 'annual', 'disproportionate', 'communicate', 'military', 'hence', 'core', 'consumer', 'facilitate', 'assure', 'reverse', 'adjustment', 'restrain', 'environment', 'formulate', 'briefly', 'institution', 'intervention', 'foundation', 'identical', 'evidential', 'legality', 'enhance', 'trigger', 'specification', 'regulatory', 'presumably', 'practitioner', 'appreciation', 'presume', 'index', 'constructive', 'analogy', 'alternatively', 'appendix', 'unaware', 'clarity', 'accurately', 'trace', 'consequently', 'adapt', 'via', 'conversion', 'reluctant', 'precedent', 'integrity', 'appropriately', 'revision', 'equate', 'occurrence', 'attain', 'inhibit', 'co-operation', 'contradict', 'pursuit', 'isolate', 'explicit', 'co-operate', 'consistency', 'classic', 'displace', 'constrain', 'dominant', 'constant', 'administrator', 'compatibility', 'flexible', 'conceive', 'evidently', 'theme', 'incentive', 'uniform', 'commentary', 'mode', 'succession', 'temporarily', 'detective', 'computation', 'regulator', 'volunteer', 'periodic', 'mature', 'controversy', 'constraint', 'mutually', 'sphere', 'evolve', 'rationality', 'journal', 'unaccompanied', 'exploit', 'tradition', 'concentration', 'retainer', 'transmission', 'survivor', 'denote', 'style', 'institutional', 'sustainable', 'enormous', 'trend', 'norm', 'cooperate', 'demonstrably', 'constituent', 'professionally', 'variable', 'abandonment', 'interpretative', 'radically', 'appreciable', 'negate', 'chart', 'reversal', 'lecture', 'estimation', 'adaptation', 'traditionally', 'random', 'medically', 'erroneously', 'linkage', 'philosophy', 'proportionately', 'exporter', 'predictable', 'legislator', 'proportional', 'convening', 'vision', 'emphasize', 'periodically', 'founder', 'selective', 'irreversible', 'subsidy', 'crucially', 'relaxation', 'instability', 'ethics', 'revelation', 'co-ordination', 'publisher', 'phenomenon', 'incapacitate', 'nuclear', 'obtainable', 'compilation', 'philosophical', 'immature', 'liberation', 'demonstrator', 'statistically', 'dynamics', 'transited', 'constitutive', 'subordination', 'thesis', 'survival', 'neutralise', 'variability', 'lecturer', 'economist', 'traceable', 'incoherent', 'resourceful', 'constitutionally', 'automate', 'unethical', 'randomness', 'constituency', 'institutionally', 'creative', 'readjust', 'expansive', 'coherently', 'undeniable', 'computational', 'unconventional', 'academy', 'redraft', 'recreate', 'constancy', 'attainable', 'conformable', 'analytically', 'unparalleled', 'restrictively', 'funder', 'expertly', 'minimize', 'chemically', 'insightful', 'unattached', 'ideological', 'unpublished', 'inaccessible', 'transferable', 'contextualize', 'schematically', 'institutionalise', 'evidence', 'authority', 'submit', 'conclusion', 'approach', 'conduct', 'identify', 'policy', 'individual', 'indicate', 'sufficient', 'significant', 'investigation', 'release', 'commission', 'community', 'assume', 'specify', 'purchase', 'scope', 'response', 'site', 'achieve', 'aspect', 'income', 'register', 'assist', 'role', 'analysis', 'criterion', 'fundamental', 'majority', 'procedural', 'impact', 'apparent', 'outcome', 'incident', 'cease', 'inconsistent', 'interpret', 'commence', 'inspector', 'specifically', 'data', 'normally', 'task', 'significance', 'notwithstanding', 'adequate', 'method', 'indication', 'sustain', 'consist', 'framework', 'presumption', 'dispose', 'restraint', 'reside', 'implication', 'series', 'contribute', 'variation', 'prohibit', 'irrelevant', 'proportionate', 'inevitably', 'occupation', 'exclusion', 'inappropriate', 'distribution', 'validity', 'disposal', 'beneficial', 'attributable', 'adult', 'extract', 'consult', 'hypothetical', 'commencement', 'imposition', 'margin', 'environmental', 'legislature', 'perceive', 'emphasis', 'approximately', 'select', 'assignment', 'assign', 'mechanism', 'transitional', 'automatically', 'technology', 'occupier', 'ministry', 'selection', 'adequately', 'indefinite', 'inadequate', 'retention', 'exclusively', 'undergo', 'survey', 'aggregate', 'construct', 'label', 'incapable', 'distribute', 'reaction', 'assistant', 'survive', 'commitment', 'locate', 'highlight', 'device', 'image', 'supplement', 'conference', 'attachment', 'levy', 'manual', 'evaluation', 'legislate', 'logical', 'evident', 'occupational', 'acknowledgement', 'flexibility', 'evaluate', 'so-called', 'restoration', 'complexity', 'portion', 'overlap', 'technique', 'logically', 'motivate', 'furthermore', 'eventual', 'predict', 'economically', 'implicate', 'successor', 'region', 'abstract', 'scenario', 'invariably', 'tape', 'authoritative', 'ratio', 'conformity', 'interaction', 'widespread', 'indicative', 'precision', 'tension', 'justifiable', 'extraction', 'principled', 'currency', 'collapse', 'technically', 'utilise', 'investor', 'focusing', 'invalidity', 'deduce', 'illogical', 'comprehensively', 'persistent', 'maturity', 'finalise', 'odds', 'unambiguous', 'ambiguous', 'compile', 'append', 'incidence', 'insufficiently', 'instructive', 'goal', 'transform', 'coincidence', 'decade', 'definitely', 'approximate', 'diversity', 'domain', 'unaffected', 'liberal', 'precedence', 'inaccuracy', 'prioritisation', 'passive', 'complement', 'reassessment', 'fluctuate', 'reformulate', 'disproportionately', 'attainment', 'deviation', 'eventuality', 'intrinsically', 'marginally', 'visual', 'reluctantly', 'persistently', 'abnormal', 'domesticate', 'theoretically', 'textual', 'abnormally', 'uniformity', 'convincingly', 'financier', 'validation', 'reformulation', 'sexuality', 'elimination', 'structurally', 'designer', 'projection', 'identically', 'restructuring', 'uniquely', 'visibility', 'incidentally', 'itemise', 'insecure', 'analytical', 'coordination', 'technological', 'dynamic', 'debatable', 'aggregation', 'preliminaries', 'securely', 'editorial', 'unaltered', 'innovative', 'unconvinced', 'misinterpretation', 'enormity', 'visualise', 'responsive', 'sequentially', 'misinterpreting', 'cyclical', 'reassess', 'intensely', 'methodical', 'predominate', 'energetically', 'psychologically', 'analyze', 'reoccur', 'rigidity', 'dominance', 'domination', 'hypothesise', 'evolutionary', 'unstructured', 'unanticipated', 'migrate', 'finalize', 'symbolic', 'coincident', 'regionally', 'uniqueness', 'illogically', 'utilization', 'indiscretion', 'transference', 'individualist', 'methodological', 'issue', 'circumstance', 'regulation', 'involve', 'legal', 'obtain', 'document', 'available', 'context', 'area', 'immigration', 'consequence', 'constitute', 'financial', 'ensure', 'medical', 'assess', 'construction', 'occur', 'amend', 'remove', 'alternative', 'function', 'amendment', 'aware', 'mental', 'consistent', 'demonstrate', 'pursue', 'cite', 'file', 'resolve', 'distinction', 'acknowledge', 'previously', 'restriction', 'option', 'status', 'estate', 'professional', 'conflict', 'sufficiently', 'imply', 'creditor', 'concept', 'positive', 'underlie', 'confine', 'legislative', 'apparently', 'decline', 'identity', 'partner', 'route', 'valid', 'somewhat', 'incompatible', 'finance', 'sex', 'initially', 'pose', 'substitute', 'alter', 'implement', 'investment', 'legally', 'ignore', 'exclusive', 'contractor', 'consultant', 'stress', 'corporate', 'guideline', 'expose', 'documentation', 'transport', 'crucial', 'prohibition', 'protocol', 'formulation', 'analyse', 'labour', 'distinct', 'ultimate', 'beneficiary', 'ethnic', 'establishment', 'discriminate', 'negative', 'reinforce', 'notion', 'insert', 'violate', 'author', 'entity', 'participation', 'compensate', 'inevitable', 'minority', 'definitive', 'parallel', 'duration', 'incline', 'psychological', 'display', 'diminish', 'inconsistency', 'intelligence', 'alteration', 'irrational', 'expertise', 'consequent', 'validly', 'reliability', 'subordinate', 'theory', 'bias', 'export', 'conclusive', 'rejection', 'exhibit', 'perception', 'supplementary', 'assembly', 'inspect', 'cycle', 'adjacent', 'psychologist', 'explicitly', 'categorise', 'gender', 'exposure', 'similarity', 'encounter', 'unjustified', 'motivation', 'illegality', 'federal', 'positively', 'substitution', 'channel', 'stability', 'depression', 'insight', 'definite', 'accessible', 'functional', 'persist', 'bulk', 'distort', 'occupant', 'compute', 'predominant', 'ministerial', 'conceivable', 'unfounded', 'rigid', 'restructure', 'theoretical', 'illegally', 'terminal', 'inconceivable', 'psychology', 'maximise', 'relocate', 'demonstration', 'culture', 'insertion', 'migrant', 'orientation', 'transmit', 'validate', 'marginal', 'conclusively', 'constantly', 'anticipation', 'economy', 'indicator', 'reliant', 'detection', 'relax', 'inflexible', 'enhancement', 'format', 'predominantly', 'intelligent', 'prediction', 'primacy', 'consultancy', 'signify', 'justifiably', 'exclusionary', 'correspondingly', 'ethnicity', 'distinctly', 'contradiction', 'occupancy', 'misinterpret', 'unify', 'distortion', 'expansion', 'integration', 'underestimate', 'equivalence', 'variant', 'specificity', 'unambiguously', 'annually', 'conceptually', 'predictability', 'advocacy', 'achievable', 'contributor', 'complementary', 'stressful','abstraction', 'differentiation', 'subsidise', 'persistence', 'unprincipled', 'unbiased', 'reactivate', 'prohibitive', 'unconstrained', 'undefined', 'co-ordinate', 'subordinates', 'mediate', 'classical', 'challenger', 'distributive', 'unprecedented', 'rigidly', 'emergence', 'facilitator', 're-evaluation', 'commodity', 'liberally', 'coordinate', 'negatively', 'unregulated', 'max', 'reactive', 'derivation', 'transitory', 'intelligently', 'professionalism', 'devotion', 'affective', 'normality', 'paralleled', 'consultative', 'inflexibility', 'bulky', 'creator', 'stylise', 'visually', 'mentality', 'exhibition', 'unalterable', 'functionally', 'accompaniment', 'diversification', 'reactor', 'ideology', 'innovator', 'creativity', 'reversible', 'unmodified', 'inconstancy', 'adaptability', 'overestimate', 'unresponsive', 'normalisation', 'proportionally', 'paragraph', 'seek', 'principle', 'respondent', 'rely', 'requirement', 'clause', 'schedule', 'proceed', 'challenge', 'commit', 'legislation', 'similar', 'secure', 'sum', 'injury', 'contrary', 'prior', 'domestic', 'define', 'definition', 'acquire', 'summary', 'primary', 'panel', 'recover', 'expert', 'element', 'physical', 'assumption', 'administrative', 'design', 'despite', 'instance', 'consent', 'reliance', 'quote', 'economic', 'resident', 'residence', 'occupy', 'capacity', 'minimum', 'registration', 'vary', 'publish', 'derive', 'albeit', 'nevertheless', 'corporation', 'terminate', 'structure', 'involvement', 'communication', 'network', 'consultation', 'relevance', 'license', 'precisely', 'estimate', 'ultimately', 'appreciate', 'revise', 'debate', 'accompany', 'contrast', 'inherent', 'purchaser', 'subsidiary', 'project', 'internal', 'invoke', 'reveal', 'team', 'emerge', 'insufficient', 'allocate', 'infer', 'computer', 'monitor', 'proportion', 'complex', 'intervene', 'consistently', 'denial', 'target', 'whereby', 'medium', 'modification', 'anticipate', 'precede', 'accurate', 'generate', 'external', 'constitutional', 'institute', 'motive', 'implementation', 'availability', 'constitution', 'clarify', 'mutual', 'citation', 'objectively', 'ongoing', 'implicit', 'initiate', 'straightforward', 'conventional', 'restore', 'grade', 'prime', 'virtually', 'traditional', 'confirmation', 'formula', 'expand', 'overseas', 'incompatibility', 'intermediate', 'undertaking', 'physically', 'fundamentally', 'compensatory', 'likewise', 'eliminate', 'colleague', 'volume', 'participant', 'voluntarily', 'sector', 'interval', 'minimise', 'successive', 'clarification', 'investigator', 'statistics', 'capability', 'sexually', 'offset', 'conform', 'financially', 'invalidate', 'cultural', 'illustration', 'hypothesis', 'inadequacy', 'ambiguity', 'minimal', 'transit', 'isolation', 'utility', 'react', 'investigative', 'distributor', 'diminution', 'coincide', 'global', 'editor', 'conceivably', 'relocation', 'energy', 'cooperation', 'promoter', 'intensive', 'coherent', 'intense', 'integrate', 'assemble', 'differentiate', 'distinctive', 'generation', 'mediation', 'indefinitely', 'rationally', 'co-operative', 'finality', 'unlicensed', 'accumulate', 'intensity', 'refine', 'internally', 'concurrently', 'immigrant', 'variance', 'methodology', 'reconstruction', 'converse', 'inappropriately', 'intrinsic', 'sufficiency', 'illustrative', 'uncontroversial', 'evolution', 'evaluative', 'diverse', 'conceptual', 'unpredictable', 'finite', 'innovation', 'imprecise', 'qualitative', 'tense', 'reconstruct', 'dominate', 'demonstrable', 'rationalisation', 'underlay', 'dramatically', 'unpredictability', 'reliably', 'conception', 'cooperative', 'transportation', 'contextual', 'objectivity', 'brevity', 'inhibition', 'accumulation', 'accessibility', 'coherence', 'appreciably', 'rationalise', 'unsustainable', 'uniformly', 'collapsible', 'approximation', 'virtual', 'intensify', 'infinitely', 'inconclusive', 'environmentally', 'unaided', 'summarize', 'selectively', 'reinforcement', 'definable', 'minimally', 'disposable', 'researcher', 'successively', 'analyst', 'redefine', 'detectable', 'predictably', 'hypothetically', 'utilize', 'globally', 'empirical', 'removable', 'revolution', 'displacement', 'unintelligent', 'ethic', 'imagery', 'innovate', 'adulthood', 'stabilise', 'immaturity', 'unscheduled', 'predominance', 'conceptualise', 'intensification', 'briefing', 'manually', 'passivity', 'discretely', 'simulation', 'coordinator', 'itemisation', 'co-ordinator', 'reactivation', 'unrestrained', 'participatory', 'unidentifiable', 'require', 'proceeding', 'benefit', 'contract', 'conclude', 'establish', 'security', 'procedure', 'impose', 'factor', 'scheme', 'reject', 'access', 'affect', 'draft', 'civil', 'discretion', 'exclude', 'fund', 'undertake', 'principal', 'maintain', 'potential', 'accommodation', 'deny', 'advocate', 'licence', 'prospect', 'assistance', 'credit', 'vehicle', 'guarantee', 'enforcement', 'instruct', 'removal', 'focus', 'equipment', 'retain', 'range', 'item', 'preliminary', 'normal', 'restrict', 'contribution', 'code', 'incorporate', 'emphasise', 'source', 'suspend', 'revenue', 'resource', 'facility', 'justification', 'chapter', 'administration', 'assurance', 'equivalent', 'precise', 'aid', 'version', 'publication', 'thereby', 'comprise', 'acquisition', 'significantly', 'injure', 'job', 'illegal', 'major', 'component', 'termination', 'identification', 'partnership', 'minor', 'suspension', 'allocation', 'discretionary', 'text', 'prospective', 'location', 'nonetheless', 'regulate', 'modify', 'abandon', 'similarly', 'bond', 'correspond', 'illustrate', 'attitude', 'accommodate', 'percentage', 'comprehensive', 'research', 'concentrate', 'convert', 'reliable', 'input', 'voluntary', 'inspection', 'plus', 'inherently', 'regional', 'rational', 'convince', 'concurrent', 'automatic', 'principally', 'detect', 'induce', 'stable', 'academic', 'edition', 'restrictive', 'topic', 'considerably', 'analogous', 'quotation', 'arbitrary', 'devote', 'shift', 'creation', 'incorporation', 'vol', 'adjust', 'sequence', 'attribution', 'erroneous', 'periodical', 'identifiable', 'logic', 'chemical', 'strategy', 'phase', 'invest', 'discrete', 'odd', 'adequacy', 'implicitly', 'promotion', 'compound', 'neutral', 'individually', 'integral', 'mentally', 'controversial', 'structural', 'accuracy', 'derivative', 'perspective', 'consensus', 'inaccurate', 'awareness', 'dimension', 'convene', 'forthcoming', 'consumption', 'ignorance', 'visible', 'output', 'appropriateness', 'reluctance', 'consume', 'contemporary', 'unique', 'strategic', 'achievement', 'radical', 'initiative', 'unspecified', 'equip', 'insignificant', 'exploitation', 'prioritise', 'depress', 'commentator', 'categorisation', 'paradigm', 'unresolved', 'unreliable', 'transition', 'interact', 'manipulate', 'infrastructure', 'parameter', 'assessable', 'unavailable', 'edit', 'ignorant', 'layer', 'dissimilar', 'contradictory', 'unrestricted', 'globe', 'hierarchy', 'refinement', 'erosion', 'neutrality', 'conventionally', 'dramatic', 'inclination', 'induction', 'statistical', 'equation', 'domestically', 'unstable', 'manipulation', 'deviate', 'schematic', 'inadequately', 'ethical', 'alternate', 'federation', 'fluctuation', 'erode', 'conversely', 'emphatically', 'optional', 'invariable', 'hierarchical', 'administratively', 'enormously', 'arbitrarily', 'arbitrariness', 'emphatic', 'economical', 're-evaluate', 'inevitability', 'detector', 'migration', 'irrelevance', 'manipulative', 'unconstitutional', 'infinite', 'instructor', 'utilisation', 'symbol', 'economics', 'orientate', 'externally', 'convertible', 'transformation', 'visibly', 'reinvest', 'initiation', 'coincidental', 'redistribution', 'academia', 'liberate', 'energetic', 'authorship', 'sustenance', 'facilitation', 'sustainability', 'orient', 'percent', 'randomly', 'assuredly', 'symbolise', 'sequential', 'academically', 'redistribute', 'individuality', 'drama', 'emergent', 'maximize', 'statistic', 'indistinct', 'unassessed', 'dimensional', 'prioritises', 'conformation', 'readjustment', 'contextualise', 'revolutionary', 'controversially']\n",
    "\n",
    "# eyeball + top 100 common\n",
    "stop_list += ['court', 'law', 'claim', 'state', 'made', 'case', 'act', 'city', 'right', 'rights', 'defendant', 'may', \n",
    "                'the', 'of', '', 'to', 'and', 'a', 'in', 'that', 'it', 'is', 'by', 'wa', 'be', 'for', 'or', 'not', 'court', 'state', 'which', 'this', 'on', 'v', 'from', 'with', 'an', 'case', 'upon', 'such', 'act', 'u', 'any', 'are', 'no', 'at', 'were', 'but', 'have', 'had', 'under', 'been', 'law', 'company', 'united', 'we', 'may', 'his', 'right', 'if', 'other', 'made', 'there', 'all', 'he', 'ha', 'said', 'would', 'one', 'question', 'so', 'statute', 'their', 'they', 'co', 'property', 'power', 'land', 'shall', 'judgment', 'tax', 'within', 'fact', 'defendant', 'order', 'only', 'against', 'time', 'when', 'should', 'contract', 'plaintiff', 'these', 'same', 'them', 'claim', 'appeal', 'purpose', 'district', 'without', 'congress', 'action', 'commission', 'new', 'provision', 'jurisdiction', 'held', 'whether', 'than', 'must', 'section', 'before', 'after', 'part', 'decision', 'suit', 'could', 'corporation', 'railroad', 'will', 'rate', 'those', 'did', 'public', 'between', 'general', 'government', 'subject', 'commerce', 'evidence', 'business', 'year', 'because', 'party', 'opinion', 'circuit', 'federal', 'who', 'where', 'into', 'person', 'effect', 'also', 'authority', 'doe', 'error', 'make', 'proceeding', 'interest', 'rule', 'amount', 'use', 'city', 'him', 'constitution', 'here', 'bank', 'do', 'first', 'more', 'being', 'out', 'interstate', 'petitioner', 'what', 'decree', 'two', 'bill', 'ground', 'value', 'can', 'sale', 'might', 'then', 'some', 'therefore', 'term', 'line', 'cannot', 'matter', 'c', 'over', 'cause', 'supreme', 'each', 'stat', 'justice', 'given', 'title', 'duty', 'service', 'present', 'further', 'provided', 'condition', 'paid', 'carrier', 'reason', 'taken', 'found', 'trial', 'brought', 'necessary', 'payment', 'board', 'mr', 'record', 'stock', 'construction', 'view', 'patent', 'thus']\n",
    "\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yujin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# sample['text']= sample['text'].apply(lambda x: [word.lower() for word in x.split() if word.lower() not in stop_list])\n",
    "sample['text'] = sample['text'].apply(lambda x: list(set(x) - stop_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yujin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "doc = sample['text'].values.tolist()\n",
    "bi = gensim.models.phrases.Phrases(doc, min_count=3, threshold=10)\n",
    "tri = gensim.models.phrases.Phrases(bi[doc], min_count=3, threshold=10)\n",
    "sample['text'] = tri[bi[doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem=sample.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1) Create dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.1) Create list from dataframe <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 lemmatise only\n",
    "doc_lem= df_lem['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.2) Create dictionary <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lem=corpora.Dictionary(doc_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id2=dict_lem.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4.1.3) TF <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_lem= [dict_lem.doc2bow(doc) for doc in doc_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA model with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) LDA with term freq (tf)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0:00:18.557808\n",
      "\n",
      "Coherence Score LDA-lem:  0.4855739242902518\n",
      "[(0,\n",
      "  '0.000*\"adjudge\" + 0.000*\"allowed_say\" + 0.000*\"orient\" + '\n",
      "  '0.000*\"constituted_burlington\" + 0.000*\"deposition_clerk\" + '\n",
      "  '0.000*\"bar_met_former_notice\" + 0.000*\"theretofore_derived\" + '\n",
      "  '0.000*\"eminent_desire\" + 0.000*\"tract_benefit\" + '\n",
      "  '0.000*\"transporting_illinois\"'),\n",
      " (1,\n",
      "  '0.001*\"ordered_virginia\" + 0.000*\"hundred_though\" + '\n",
      "  '0.000*\"application_transfer_transferred\" + 0.000*\"rev_portion\" + '\n",
      "  '0.000*\"refused_march_final\" + 0.000*\"offence_constitutional\" + '\n",
      "  '0.000*\"involved_herein\" + 0.000*\"exception_constitutionality\" + '\n",
      "  '0.000*\"superior_american\" + 0.000*\"meandered\"'),\n",
      " (2,\n",
      "  '0.000*\"counsel_money\" + 0.000*\"proper\" + 0.000*\"allowed_true\" + '\n",
      "  '0.000*\"principle\" + 0.000*\"proof_appear\" + 0.000*\"claimed_follows\" + '\n",
      "  '0.000*\"defence_directly\" + 0.000*\"several_legal\" + 0.000*\"show\" + '\n",
      "  '0.000*\"properly_find\"'),\n",
      " (3,\n",
      "  '0.001*\"sustained_sued\" + 0.001*\"qb\" + 0.001*\"baron\" + 0.001*\"delicto\" + '\n",
      "  '0.000*\"contended_extra\" + 0.000*\"elizabeth\" + 0.000*\"entitled\" + '\n",
      "  '0.000*\"indirectly_page\" + 0.000*\"alleged\" + 0.000*\"attorney\"'),\n",
      " (4,\n",
      "  '0.001*\"alleged\" + 0.001*\"affirmed\" + 0.001*\"directly\" + 0.001*\"writ\" + '\n",
      "  '0.001*\"filed\" + 0.001*\"decided\" + 0.001*\"however\" + 0.001*\"entered\" + '\n",
      "  '0.001*\"set\" + 0.001*\"sustained\"'),\n",
      " (5,\n",
      "  '0.000*\"expressly_consideration\" + 0.000*\"whose_conclusion\" + '\n",
      "  '0.000*\"render_though\" + 0.000*\"solely_expressed\" + 0.000*\"settled_except\" + '\n",
      "  '0.000*\"maintained_charge\" + 0.000*\"considering\" + '\n",
      "  '0.000*\"discretion_counsel\" + 0.000*\"operating_although\" + '\n",
      "  '0.000*\"required_course\"'),\n",
      " (6,\n",
      "  '0.001*\"mind_deed\" + 0.001*\"answer_next\" + 0.001*\"continued_writ\" + '\n",
      "  '0.001*\"either_cited\" + 0.001*\"treaty_providing\" + '\n",
      "  '0.001*\"repugnant_involved\" + 0.000*\"called_entering\" + '\n",
      "  '0.000*\"limit_belonging\" + 0.000*\"affirmed\" + 0.000*\"foreign_association\"'),\n",
      " (7,\n",
      "  '0.000*\"arise_exempt\" + 0.000*\"however_affixed\" + 0.000*\"many\" + '\n",
      "  '0.000*\"called\" + 0.000*\"length\" + 0.000*\"well\" + 0.000*\"hold\" + '\n",
      "  '0.000*\"forth_secretary\" + 0.000*\"though\" + 0.000*\"hand\"'),\n",
      " (8,\n",
      "  '0.000*\"bona_county\" + 0.000*\"assignor_disposed\" + 0.000*\"admitted_perhaps\" '\n",
      "  '+ 0.000*\"exercised_indeed\" + 0.000*\"even_suppose\" + 0.000*\"stated\" + '\n",
      "  '0.000*\"approbation\" + 0.000*\"say\" + 0.000*\"china\" + 0.000*\"perrin\"'),\n",
      " (9,\n",
      "  '0.000*\"conrad\" + 0.000*\"language_always\" + 0.000*\"mcgourkey\" + '\n",
      "  '0.000*\"ferryboat\" + 0.000*\"palmyra\" + 0.000*\"fanning\" + 0.000*\"railway\" + '\n",
      "  '0.000*\"alleged\" + 0.000*\"derived_reference_appropriate_impose\" + '\n",
      "  '0.000*\"true_ignorance\"')]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "lda_reviews_lem= gensim.models.ldamodel.LdaModel(corpus=vec_lem, id2word=dict_lem, num_topics=10, random_state=100, alpha=0.001)\n",
    "print(\"Time taken: \", datetime.datetime.now()-start)\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence score for lemmatising -tf only\n",
    "coherence_model_lda_lem = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "coherence_lda_lem = coherence_model_lda_lem.get_coherence()\n",
    "print('\\nCoherence Score LDA-lem: ', coherence_lda_lem)\n",
    "\n",
    "from pprint import pprint\n",
    "optimal_model = lda_reviews_lem\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_lem=lda_reviews_lem.show_topics(12,15)\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     print(topics_lem[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign topic back to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem['topic'] = [optimal_model.get_document_topics(dict_lem.doc2bow(doc)) for doc in doc_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>category</th>\n",
       "      <th>per_curiam</th>\n",
       "      <th>case_name</th>\n",
       "      <th>date_filed</th>\n",
       "      <th>federal_cite_one</th>\n",
       "      <th>absolute_url</th>\n",
       "      <th>cluster</th>\n",
       "      <th>year_filed</th>\n",
       "      <th>scdb_id</th>\n",
       "      <th>scdb_decision_direction</th>\n",
       "      <th>scdb_votes_majority</th>\n",
       "      <th>scdb_votes_minority</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22969</td>\n",
       "      <td>Justice Fuller</td>\n",
       "      <td>majority</td>\n",
       "      <td>False</td>\n",
       "      <td>Fisher v. Baker</td>\n",
       "      <td>2010-10-09</td>\n",
       "      <td>203 U.S. 174</td>\n",
       "      <td>https://www.courtlistener.com/opinion/1447389/...</td>\n",
       "      <td>https://www.courtlistener.com/api/rest/v3/clus...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1906-017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[alleged_counsel_basis, amended_wright, motion...</td>\n",
       "      <td>[(4, 0.8799718), (5, 0.11997856)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author_name  category  per_curiam        case_name  date_filed  \\\n",
       "22969  Justice Fuller  majority       False  Fisher v. Baker  2010-10-09   \n",
       "\n",
       "      federal_cite_one                                       absolute_url  \\\n",
       "22969     203 U.S. 174  https://www.courtlistener.com/opinion/1447389/...   \n",
       "\n",
       "                                                 cluster  year_filed  \\\n",
       "22969  https://www.courtlistener.com/api/rest/v3/clus...        2010   \n",
       "\n",
       "        scdb_id  scdb_decision_direction  scdb_votes_majority  \\\n",
       "22969  1906-017                      1.0                  8.0   \n",
       "\n",
       "       scdb_votes_minority                                               text  \\\n",
       "22969                  0.0  [alleged_counsel_basis, amended_wright, motion...   \n",
       "\n",
       "                                   topic  \n",
       "22969  [(4, 0.8799718), (5, 0.11997856)]  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem.head(1)\n",
    "# possible topic of 4 and 5, adding probability to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----END----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Optimal Number of the Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Model with tf vectors - DONT RUN THIS<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run. In this case we are going to  k_max=10.\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "model_list = []\n",
    "coherence_values = []\n",
    "model_topics = []\n",
    "\n",
    "for num_topics in range(2, 14, 2):\n",
    "    #sg_lda_x = gensim.models.ldamodel.LdaModel(corpus=sg_vecs, id2word=sg_dictionary, num_topics=num_topics)\n",
    "    lda_reviews_lem= gensim.models.ldamodel.LdaModel(corpus=vec_lem, id2word=dict_lem, num_topics=num_topics, iterations=1000, random_state=100)\n",
    "    coherencemodel = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "    model_topics.append(num_topics)\n",
    "    model_list.append(lda_reviews_lem)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    print(\"#Topics: \" + str(num_topics) + \" Score: \" + str(coherencemodel.get_coherence()))\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "limit=14; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>- END HERE BECAUSE RESULT ARE NOT GOOD-<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Score - dont think need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence score for lemmatising -tf only\n",
    "coherence_model_lda_lem = CoherenceModel(model=lda_reviews_lem, texts=doc_lem, dictionary=dict_lem, coherence='c_v')\n",
    "coherence_lda_lem = coherence_model_lda_lem.get_coherence()\n",
    "print('\\nCoherence Score LDA-lem: ', coherence_lda_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "optimal_model = lda_reviews_lem\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Dominant Topic for each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most dominant topic\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(data)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "#I choose model_list[1] where the number of topics is 4\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=vec_lem, data=doc_lem)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a model to disk, or reload a pre-trained model\n",
    "import pickle\n",
    "\n",
    "# lda_lem_tfidf.save(\"lda\")\n",
    "\n",
    "ldapickle = open('ldapickle', \"wb\")\n",
    "pickle.dump(optimal_model, ldapickle)\n",
    "ldapickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "reader = open(\"ldamallet_model478927.pickle\", \"rb\")\n",
    "tester_model = pickle.load(reader)\n",
    "topics_lem=tester_model.show_topics(10,15)\n",
    "topics_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NEW DATA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list += ['hotel', 'however', 'could', 'get', 'back', 'bit', 'one', 'know', 'i', 'have', 'would', 'take', 'a', 'choose', 'the', 'first', 'second', 'lovely', 'will', 'definitely', 'longer', 'stayed', 'also']\n",
    "\n",
    "def preprocessing(review):\n",
    "    sentences = review.split(\". \")\n",
    "    data = [[word.lower() for word in x.split() if word.lower() not in stop_list] for x in sentences]\n",
    "#     lem = [[lemmatizer.lemmatize(w) for w in doc] for doc in data]\n",
    "    dict_lem=corpora.Dictionary(data)\n",
    "    token_to_id2=dict_lem.token2id\n",
    "    vec_lem= [dict_lem.doc2bow(doc) for doc in data]\n",
    "    \n",
    "    return vec_lem\n",
    "\n",
    "unseen_rev= preprocessing(\"room and bed is spacious. shower old put room water. staff friendly reception room helpful bar.\")\n",
    "\n",
    "for sen in unseen_rev:\n",
    "#     tester_model is the lda model that you load with pickle\n",
    "    result=tester_model[sen]\n",
    "    result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    topic = result[0][0]\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "classifier_saved = open(\"ldamallet_model478927.pickle\", \"rb\") #binary read\n",
    "tester_model = pickle.load(classifier_saved)\n",
    "classifier_saved.close()\n",
    "\n",
    "# TEST NEW DATA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list += ['hotel', 'however', 'could', 'get', 'back', 'bit', 'one', 'know', 'i', 'have', 'would', 'take', 'a', 'choose', 'the', 'first', 'second', 'lovely', 'will', 'definitely', 'longer', 'stayed', 'also']\n",
    "\n",
    "def preprocessing(review):\n",
    "    sentences = review.split(\". \")\n",
    "    data = [[word.lower() for word in x.split() if word.lower() not in stop_list] for x in sentences]\n",
    "#     lem = [[lemmatizer.lemmatize(w) for w in doc] for doc in data]\n",
    "    dict_lem=corpora.Dictionary(data)\n",
    "    token_to_id2=dict_lem.token2id\n",
    "    vec_lem= [dict_lem.doc2bow(doc) for doc in data]\n",
    "    \n",
    "    return vec_lem\n",
    "\n",
    "unseen_rev= preprocessing(\"room and bed is spacious. shower old put room water. staff friendly reception room helpful bar.\")\n",
    "\n",
    "gensim_lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(tester_model)\n",
    "\n",
    "# print(unseen_rev)\n",
    "for sen in unseen_rev:\n",
    "#     tester_model is the lda model that you load with pickle\n",
    "    result=gensim_lda[sen]\n",
    "    result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    topic = result[0][0]\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
